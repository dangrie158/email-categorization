{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorization using averaged word vectors as document feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:19:36.521368",
     "start_time": "2017-01-14T21:19:34.784893"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "stopwords = sw.words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a list of:\n",
    "* the full corpora split into categories -> ```fulldata_path```\n",
    "* a subset of each category corpus used for training -> ```train_paths```\n",
    "* a subset of each category corpus used for validation -> ```validation_paths```\n",
    "\n",
    "the categories were split into training / validation by using the ```mail/generateSets.py``` script with a 70 / 30 split between training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:20:21.984864",
     "start_time": "2017-01-14T21:19:48.658227"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_names = ['Sonstiges', 'Aktuell', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.200dim.word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:20:43.888260",
     "start_time": "2017-01-14T21:20:43.881254"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split() if x not in stopwords]\n",
    "                X.append(tokens)\n",
    "                y.append(name)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec Vectorizers\n",
    "These vectorizers are used to transform a set of vectors to a single vector. They are used to transform a list of word embeddings to a single vector that represents the whole article.\n",
    "\n",
    "Both variations simply build the average of all word-vectors. The TFIDF variation however uses the word frequency and inverse-document frequency to weight the word vectors.\n",
    "\n",
    "The implementation in mostly adapted from [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) by Nadbor Drozd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```MeanEmbeddingVectorizer``` generates a document vector $  \\overrightarrow { d } $ from a list of word vectors by calculating\n",
    "\n",
    "$$ \\overrightarrow { d } =\\frac { \\sum _{ i=0 }^{ dim(d) }{ \\overrightarrow { { w }_{ d,i } }  }  }{ dim(d) } $$\n",
    "\n",
    "where: \n",
    "* $ \\overrightarrow { {w}_{d,i} } $ is the $ i $-th word of document $ d $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:20:49.480146",
     "start_time": "2017-01-14T21:20:49.472381"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```TfidfEmbeddingVectorizer``` uses the same averaging strategy as the ```MeanEmbeddingVectorizer```, however it also weights every word vector $ \\overrightarrow { {w}_{d,i} }$ with the term frequency-inverse document frequency (TF-IDF) of the word to put more weight on words appearing in fewer documents.\n",
    "\n",
    "$$ \\overrightarrow { d } =\\frac { \\sum _{ i=0 }^{ dim(d) }{ \\overrightarrow { { w }_{ d,i } } *tfidf(\\overrightarrow { { w }_{ d,i } } ) }  }{ dim(d) }  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:20:50.963659",
     "start_time": "2017-01-14T21:20:50.943173"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = word2vec.vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a simple random forest classifier is used for classification of the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:20:52.721794",
     "start_time": "2017-01-14T21:20:52.716978"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(base_model)), \n",
    "                        (\"random forest\", RandomForestClassifier(n_estimators=200))])\n",
    "rf_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(base_model)), \n",
    "                        (\"random forest\", RandomForestClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation Score\n",
    "\n",
    "in this section, the cross_val_score function of scikitlearn is used to validate the model. \n",
    "\n",
    "However, to be able to equally compare the different classification strategies, a fixed training and validation set is used in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:23:55.833583",
     "start_time": "2017-01-14T21:20:54.179908"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X, y = load_sets(fulldata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:28:55.614610",
     "start_time": "2017-01-14T21:24:07.542877"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_rf = cross_val_score(rf_w2v, X, y, cv=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:42:06.601846",
     "start_time": "2017-01-14T21:29:22.924152"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_rf_tfidf = cross_val_score(rf_w2v_tfidf, X, y, cv=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:43:41.414891",
     "start_time": "2017-01-14T21:43:41.404174"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score simple: 0.59461255141\n",
      "Score TFIDF:  0.595011215514\n"
     ]
    }
   ],
   "source": [
    "print('Score simple: {}'.format(score_rf))\n",
    "print('Score TFIDF:  {}'.format(score_rf_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:43:43.363018",
     "start_time": "2017-01-14T21:43:43.357121"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use only the tfidf model for further consideration since it performs slightly better in the cross validation\n",
    "# however, it also needs twice the time to compute\n",
    "\n",
    "# create a new instance to make sure the model isn't pre trained from the previous step\n",
    "test_model =  Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(base_model)), \n",
    "                        (\"extra trees\", RandomForestClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:45:50.814738",
     "start_time": "2017-01-14T21:43:44.869162"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = load_sets(train_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:52:24.764802",
     "start_time": "2017-01-14T21:46:11.311137"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit returns self. assign it to a dummy variable to stop jupyter from printing the model\n",
    "_ = test_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:54:16.662124",
     "start_time": "2017-01-14T21:53:14.010092"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "validate_X, validate_y = load_sets(validation_paths)\n",
    "predicted_y = test_model.predict(validate_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "This section performs the same validation steps that were used when validating the log-likelihood score approach fpr article classification, so the steps aren't as well documented. Thee the other document for a complete explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:54:34.447881",
     "start_time": "2017-01-14T21:54:34.389856"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Sonstiges  Aktuell  Lifestyle  Wirtschaft  Finanzen  Ausland  \\\n",
      "Sonstiges          509        3        131          30        10       28   \n",
      "Aktuell              0        0          0           0         0        0   \n",
      "Lifestyle           14        0         87           9         0        2   \n",
      "Wirtschaft          39        5         47         615       133       27   \n",
      "Finanzen             0        0          5           6       162        1   \n",
      "Ausland              4        0          1           2         2       71   \n",
      "Lokal                4        0          0           0         1        0   \n",
      "Politik           1027       28        387        1183       390      900   \n",
      "Sport               13        0          1           1         0        2   \n",
      "Technologie         24        4          7          39         4        1   \n",
      "Kultur               8        0         12           1         0        2   \n",
      "\n",
      "             Lokal  Politik  Sport  Technologie  Kultur  \n",
      "Sonstiges       2"
     ]
    }
   ],
   "source": [
    "classification_matrix = np.zeros([num_models, num_models], dtype=int)\n",
    "\n",
    "for target, predicted in zip(validate_y, predicted_y):\n",
    "    target_index = category_names.index(target)\n",
    "    predicted_index = category_names.index(predicted)\n",
    "    classification_matrix[predicted_index, target_index] += 1\n",
    "    \n",
    "result = DataFrame(classification_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-15T09:57:44.332210",
     "start_time": "2016-12-15T09:57:44.328750"
    }
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:54:36.124725",
     "start_time": "2017-01-14T21:54:36.103391"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Sonstiges   Aktuell  Lifestyle  Wirtschaft  Finanzen   Ausland  \\\n",
      "Sonstiges     0.566185  0.003337   0.145717    0.033370  0.011123  0.031146   \n",
      "Aktuell       0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
      "Lifestyle     0.095238  0.000000   0.591837    0.061224  0.000000  0.013605   \n",
      "Wirtschaft    0.037827  0.004850   0.045587    0.596508  0.129001  0.026188   \n",
      "Finanzen      0.000000  0.000000   0.028409    0.034091  0.920455  0.005682   \n",
      "Ausland       0.030769  0.000000   0.007692    0.015385  0.015385  0.546154   \n",
      "Lokal         0.028777  0.000000   0.000000    0.000000  0.007194  0.000000   \n",
      "Politik       0.118879  0.003241   0.044797    0.136937  0.045144  0.104179   \n",
      "Sport         0.028953  0.000000   0.002227    0.002227  0.000000  0.004454   \n",
      "Technologie   0.071006  0.011834   0.020710    0.115385  0.011834  0.002959   \n",
      "Kultur        0.085106  0.000000   0.127660    0.010638  0.000000  0.021277   \n",
      "\n",
      "                Lokal   Politik     Sport  Technolo"
     ]
    }
   ],
   "source": [
    "# the max(, 1) function surrounding sum makes sure wo don't divide by 0 if no match occurred\n",
    "accuracy_matrix = [category / float(max([sum(category) ,1])) for category in classification_matrix]\n",
    "\n",
    "result = DataFrame(accuracy_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:54:36.971256",
     "start_time": "2017-01-14T21:54:36.965273"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.464457731274\n"
     ]
    }
   ],
   "source": [
    "true_positives = 0.0\n",
    "num_samples = 0\n",
    "for x in range(num_models):\n",
    "    true_positives += classification_matrix[x][x]\n",
    "    num_samples += sum(classification_matrix[x])\n",
    "    \n",
    "average_score = true_positives / num_samples\n",
    "\n",
    "print('score: {}'.format(average_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-14T21:54:42.640880",
     "start_time": "2017-01-14T21:54:42.629434"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.464457731274\n"
     ]
    }
   ],
   "source": [
    "# make suer we calculate the \"same\" accuracy as scipy\n",
    "# just to prevent dumb mistakes...\n",
    "score = accuracy_score(validate_y, predicted_y, normalize=True)\n",
    "print('score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T17:08:44.100956",
     "start_time": "2016-12-17T17:08:44.095428"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
