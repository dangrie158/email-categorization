{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorization using averaged word vectors as document feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:37:54.329973",
     "start_time": "2016-12-17T16:37:54.321287"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from pandas import DataFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "stopwords = sw.words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a list of:\n",
    "* the full corpora split into categories -> ```fulldata_path```\n",
    "* a subset of each category corpus used for training -> ```train_paths```\n",
    "* a subset of each category corpus used for validation -> ```validation_paths```\n",
    "\n",
    "the categories were split into training / validation by using the ```mail/generateSets.py``` script with a 70 / 30 split between training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:38:42.548934",
     "start_time": "2016-12-17T16:37:55.824302"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category_names = ['Sonstiges', 'Aktuell', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:38:42.566500",
     "start_time": "2016-12-17T16:38:42.554047"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split() if x not in stopwords]\n",
    "                X.append(tokens)\n",
    "                y.append(name)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec Vectorizers\n",
    "These vectorizers are used to transform a set of vectors to a single vector. They are used to transform a list of word embeddings to a single vector that represents the whole article.\n",
    "\n",
    "Both variations simply build the average of all word-vectors. The TFIDF variation however uses the word frequency and inverse-document frequency to weight the word vectors.\n",
    "\n",
    "The implementation in mostly adapted from [Text Classification With Word2Vec](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) by Nadbor Drozd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```MeanEmbeddingVectorizer``` generates a document vector $  \\overrightarrow { d } $ from a list of word vectors by calculating\n",
    "\n",
    "$$ \\overrightarrow { d } =\\frac { \\sum _{ i=0 }^{ dim(d) }{ \\overrightarrow { { w }_{ d,i } }  }  }{ dim(d) } $$\n",
    "\n",
    "where: \n",
    "* $ \\overrightarrow { {w}_{d,i} } $ is the $ i $-th word of document $ d $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:38:42.582801",
     "start_time": "2016-12-17T16:38:42.571057"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = word2vec.vector_size\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] \n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```TfidfEmbeddingVectorizer``` uses the same averaging strategy as the ```MeanEmbeddingVectorizer```, however it also weights every word vector $ \\overrightarrow { {w}_{d,i} }$ with the term frequency-inverse document frequency (TF-IDF) of the word to put more weight on words appearing in fewer documents.\n",
    "\n",
    "$$ \\overrightarrow { d } =\\frac { \\sum _{ i=0 }^{ dim(d) }{ \\overrightarrow { { w }_{ d,i } } *tfidf(\\overrightarrow { { w }_{ d,i } } ) }  }{ dim(d) }  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:38:48.641120",
     "start_time": "2016-12-17T16:38:48.620810"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = word2vec.vector_size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a simple random forest classifier is used for classification of the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:38:50.984656",
     "start_time": "2016-12-17T16:38:50.976717"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_w2v = Pipeline([(\"word2vec vectorizer\", MeanEmbeddingVectorizer(base_model)), \n",
    "                        (\"random forest\", RandomForestClassifier(n_estimators=200))])\n",
    "rf_w2v_tfidf = Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(base_model)), \n",
    "                        (\"random forest\", RandomForestClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation Score\n",
    "\n",
    "in this section, the cross_val_score function of scikitlearn is used to validate the model. \n",
    "\n",
    "However, to be able to equally compare the different classification strategies, a fixed training and validation set is used in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:41:19.035934",
     "start_time": "2016-12-17T16:38:53.084951"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "X, y = load_sets(fulldata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:47:31.406427",
     "start_time": "2016-12-17T16:41:19.037385"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_rf = cross_val_score(rf_w2v, X, y, cv=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:48:12.281846",
     "start_time": "2016-12-17T16:47:31.408656"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_rf_tfidf = cross_val_score(rf_w2v_tfidf, X, y, cv=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:48:12.335227",
     "start_time": "2016-12-17T16:48:12.286306"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score simple: 0.596355703114\n",
      "Score TFIDF:  0.592372507043\n"
     ]
    }
   ],
   "source": [
    "print('Score simple: {}'.format(score_rf))\n",
    "print('Score TFIDF:  {}'.format(score_rf_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:48:23.726346",
     "start_time": "2016-12-17T16:48:23.718281"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use only the tfidf model for further consideration since it performs slightly better in the cross validation\n",
    "# however, it also needs twice the time to compute\n",
    "\n",
    "# create a new instance to make sure the model isn't pre trained from the previous step\n",
    "test_model =  Pipeline([(\"word2vec vectorizer\", TfidfEmbeddingVectorizer(base_model)), \n",
    "                        (\"extra trees\", RandomForestClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:50:19.139225",
     "start_time": "2016-12-17T16:48:25.873183"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = load_sets(train_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:57:53.039043",
     "start_time": "2016-12-17T16:51:26.593591"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit returns self. assign it to a dummy variable to stop jupyter from printing the model\n",
    "_ = test_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T16:59:25.174406",
     "start_time": "2016-12-17T16:58:28.262778"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "validate_X, validate_y = load_sets(validation_paths)\n",
    "predicted_y = test_model.predict(validate_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "This section performs the same validation steps that were used when validating the log-likelihood score approach fpr article classification, so the steps aren't as well documented. Thee the other document for a complete explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T17:00:10.519313",
     "start_time": "2016-12-17T17:00:10.416121"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Sonstiges  Aktuell  Lifestyle  Wirtschaft  Finanzen  Ausland  \\\n",
      "Sonstiges          505        3        142          36        11       30   \n",
      "Aktuell              0        0          0           0         0        0   \n",
      "Lifestyle           18        0         79           8         0        2   \n",
      "Wirtschaft          39        7         50         631       129       31   \n",
      "Finanzen             0        0          2           8       162        1   \n",
      "Ausland              1        0          1           1         0       67   \n",
      "Lokal                4        0          0           0         0        0   \n",
      "Politik           1034       26        388        1163       397      900   \n",
      "Sport                8        0          0           1         0        2   \n",
      "Technologie         19        4          5          37         3        1   \n",
      "Kultur              14        0         11           1         0        0   \n",
      "\n",
      "             Lokal  Politik  Sport  Technologie  Kultur  \n",
      "Sonstiges       28       56     10           37      77  \n",
      "Aktuell          0        0      0            0       0  \n",
      "Lifestyle        2        0      0            8      17  \n",
      "Wirtschaft      24       81      3           54       3  \n",
      "Finanzen         0        1      1            0       0  \n",
      "Ausland          9       31      1            0       3  \n",
      "Lokal          133        0      0            0       1  \n",
      "Politik        301     3301    478          403     243  \n",
      "Sport            2        1    424            0       2  \n",
      "Technologie      2        3      1          239      20  \n",
      "Kultur           1        0      0            3      56  \n"
     ]
    }
   ],
   "source": [
    "classification_matrix = np.zeros([num_models, num_models], dtype=int)\n",
    "\n",
    "for target, predicted in zip(validate_y, predicted_y):\n",
    "    target_index = category_names.index(target)\n",
    "    predicted_index = category_names.index(predicted)\n",
    "    classification_matrix[predicted_index, target_index] += 1\n",
    "    \n",
    "result = DataFrame(classification_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-15T09:57:44.332210",
     "start_time": "2016-12-15T09:57:44.328750"
    }
   },
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T17:00:12.540040",
     "start_time": "2016-12-17T17:00:12.503553"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Sonstiges   Aktuell  Lifestyle  Wirtschaft  Finanzen   Ausland  \\\n",
      "Sonstiges     0.540107  0.003209   0.151872    0.038503  0.011765  0.032086   \n",
      "Aktuell       0.000000  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
      "Lifestyle     0.134328  0.000000   0.589552    0.059701  0.000000  0.014925   \n",
      "Wirtschaft    0.037072  0.006654   0.047529    0.599810  0.122624  0.029468   \n",
      "Finanzen      0.000000  0.000000   0.011429    0.045714  0.925714  0.005714   \n",
      "Ausland       0.008772  0.000000   0.008772    0.008772  0.000000  0.587719   \n",
      "Lokal         0.028986  0.000000   0.000000    0.000000  0.000000  0.000000   \n",
      "Politik       0.119759  0.003011   0.044939    0.134700  0.045981  0.104239   \n",
      "Sport         0.018182  0.000000   0.000000    0.002273  0.000000  0.004545   \n",
      "Technologie   0.056886  0.011976   0.014970    0.110778  0.008982  0.002994   \n",
      "Kultur        0.162791  0.000000   0.127907    0.011628  0.000000  0.000000   \n",
      "\n",
      "                Lokal   Politik     Sport  Technologie    Kultur  \n",
      "Sonstiges    0.029947  0.059893  0.010695     0.039572  0.082353  \n",
      "Aktuell      0.000000  0.000000  0.000000     0.000000  0.000000  \n",
      "Lifestyle    0.014925  0.000000  0.000000     0.059701  0.126866  \n",
      "Wirtschaft   0.022814  0.076996  0.002852     0.051331  0.002852  \n",
      "Finanzen     0.000000  0.005714  0.005714     0.000000  0.000000  \n",
      "Ausland      0.078947  0.271930  0.008772     0.000000  0.026316  \n",
      "Lokal        0.963768  0.000000  0.000000     0.000000  0.007246  \n",
      "Politik      0.034862  0.382326  0.055363     0.046676  0.028145  \n",
      "Sport        0.004545  0.002273  0.963636     0.000000  0.004545  \n",
      "Technologie  0.005988  0.008982  0.002994     0.715569  0.059880  \n",
      "Kultur       0.011628  0.000000  0.000000     0.034884  0.651163  \n"
     ]
    }
   ],
   "source": [
    "# the max(, 1) function surrounding sum makes sure wo don't divide by 0 if no match occurred\n",
    "accuracy_matrix = [category / float(max([sum(category) ,1])) for category in classification_matrix]\n",
    "\n",
    "result = DataFrame(accuracy_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T17:07:45.327735",
     "start_time": "2016-12-17T17:07:45.321696"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.46478990201\n"
     ]
    }
   ],
   "source": [
    "true_positives = 0.0\n",
    "num_samples = 0\n",
    "for x in range(num_models):\n",
    "    true_positives += classification_matrix[x][x]\n",
    "    num_samples += sum(classification_matrix[x])\n",
    "    \n",
    "average_score = true_positives / num_samples\n",
    "\n",
    "print('score: {}'.format(average_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T17:08:31.523883",
     "start_time": "2016-12-17T17:08:31.514363"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.46478990201\n"
     ]
    }
   ],
   "source": [
    "# make suer we calculate the \"same\" accuracy as scipy\n",
    "# just to prevent dumb mistakes...\n",
    "score = accuracy_score(validate_y, predicted_y, normalize=True)\n",
    "print('score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-17T17:08:44.100956",
     "start_time": "2016-12-17T17:08:44.095428"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# phew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
