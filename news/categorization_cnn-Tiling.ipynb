{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a CNN for news topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:25:50.357664",
     "start_time": "2017-01-21T00:25:50.347080"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set this to true to learn the model when running. false will load the model from disk\n",
    "LEARN=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:25:52.913714",
     "start_time": "2017-01-21T00:25:51.016342"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:26:28.387990",
     "start_time": "2017-01-21T00:25:52.915436"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel has 200 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "category_names = ['Sonstiges', 'Aktuell', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.200dim.word2vec.model')\n",
    "\n",
    "# we can precompute the L2-normalized vectors to save lots of memory\n",
    "# we can't continue learning after they are normalized but the model is static\n",
    "# in this usecase anyways\n",
    "base_model.init_sims(replace=True)\n",
    "\n",
    "k = base_model.vector_size\n",
    "print(\"basemodel has {} dimensional vectors\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** stopword filtering is commented out, check if filtering stopwords improves perfomance, but since the cnn learns \"patterns\" the filtering may distort the pattern too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:26:30.204109",
     "start_time": "2017-01-21T00:26:30.197003"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split()]# if x not in stopwords]\n",
    "                if len(tokens) > 0:\n",
    "                    X.append(tokens)\n",
    "                    y.append(name)\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-25T11:26:15.096608",
     "start_time": "2016-12-25T11:21:21.000650"
    },
    "collapsed": true
   },
   "source": [
    "load the raw train and validation datasets in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:26:32.963385",
     "start_time": "2017-01-21T00:26:31.710547"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 13188 articles\n",
      "loaded 5614 articles\n"
     ]
    }
   ],
   "source": [
    "raw_train_X, raw_train_y = load_sets(train_paths)\n",
    "raw_validation_X, raw_validation_y = load_sets(validation_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the average article length $N$\n",
    "\n",
    "$$N_{avg}=\\frac { \\sum _{ x\\in X }^{  }{ dim(x) }  }{ dim(X) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:26:34.039254",
     "start_time": "2017-01-21T00:26:34.018188"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average article length is: 476 words\n",
      "-95\n"
     ]
    }
   ],
   "source": [
    "N_avg = sum([len(article) for article in raw_train_X]) / len(raw_train_X)\n",
    "print(\"average article length is: {} words\".format(N_avg))\n",
    "\n",
    "#override to avoid out-of-mem errors while learning\n",
    "N_avg = 200\n",
    "\n",
    "padding = [len(article) - N_avg for article in raw_train_X if len(article) - N_avg < 0]\n",
    "average_padding = sum(padding) / len(padding)\n",
    "print(average_padding)\n",
    "#print(padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-31T10:43:14.290958",
     "start_time": "2016-12-31T10:43:14.286344"
    }
   },
   "source": [
    "convert the raw input article to matrices of word-vectors.\n",
    "each article $x$ is represented as\n",
    "\n",
    "$${ x }_{ 1:n }={ x }_{ 1 }\\oplus { x }_{ 2 }\\oplus \\dots \\oplus { x }_{ n }$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ i }\\in { { R } }^{ k }$ is the $k$-dimensional word-embedding vector for the $i$-th word in the article.\n",
    "* $\\oplus$ is the concatenation operator\n",
    "\n",
    "the result is a matrix for each article in the form:\n",
    "$$x=\\begin{bmatrix} { x }_{ 1,1 } & { x }_{ 1,2 } & \\cdots  & { x }_{ 1,k } \\\\ { x }_{ 2,1 } & { x }_{ 2,2 } & \\cdots  & { x }_{ 2,k } \\\\ \\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ { x }_{ n,1 } & { x }_{ n,2 } & \\cdots  & { x }_{ n,k } \\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ n,k }$ is the value of the $k$-th dimension of the word-vector for word $n$\n",
    "\n",
    "the matrix is padded or cropped to a length of $N_{avg}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:35:10.098789",
     "start_time": "2017-01-21T00:34:55.894001"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 255864 of 2637600 words (9.70063694268%)\n",
      "skipped 108097 of 1122800 words (9.62744923406%)\n"
     ]
    }
   ],
   "source": [
    "def articles_to_matrices(articles, word_dim, article_len):\n",
    "    X = np.zeros((len(articles), article_len, word_dim), dtype='float32')\n",
    "    words_found = 0\n",
    "    words = 0\n",
    "    for x, raw_article in enumerate(articles):    \n",
    "        for x_n in range(N_avg):#while (words_found < N_avg):\n",
    "            # if the word can't be found, use zero vector\n",
    "            word_vec = np.zeros(word_dim)\n",
    "\n",
    "            # try to load the word from the basemodel\n",
    "            # TODO: maybe skip non-available words rather than default-zero\n",
    "            # so if more then N_avg words ar in the article they get used\n",
    "            try:\n",
    "                word_vec = base_model[raw_article[x_n % len(raw_article)]]\n",
    "                words_found += 1\n",
    "                X[x, x_n] = word_vec\n",
    "            except:\n",
    "                pass\n",
    "            words += 1\n",
    "    \n",
    "    words_skipped = words - words_found\n",
    "    print(\"skipped {} of {} words ({}%)\".format(words_skipped, words, words_skipped*100.0 / words))\n",
    "    return X\n",
    "\n",
    "train_X = articles_to_matrices(raw_train_X, k, N_avg)\n",
    "validation_X = articles_to_matrices(raw_validation_X, k, N_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the string train input data to a one-hot vector that can be used on the output layer of the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:35:25.202577",
     "start_time": "2017-01-21T00:35:25.163732"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             dim(V_i)\n",
      "Aktuell            20\n",
      "Ausland           483\n",
      "Finanzen          333\n",
      "Kultur            208\n",
      "Lifestyle         328\n",
      "Lokal             249\n",
      "Politik          1596\n",
      "Sonstiges         770\n",
      "Sport             449\n",
      "Technologie       361\n",
      "Wirtschaft        817\n"
     ]
    }
   ],
   "source": [
    "def categories_to_one_hot(categories):\n",
    "    category_names, int_y = np.unique(categories, return_inverse=True)\n",
    "    y = np_utils.to_categorical(int_y)\n",
    "    return y, category_names\n",
    "\n",
    "train_y, _ = categories_to_one_hot(raw_train_y)\n",
    "target_y, category_names = categories_to_one_hot(raw_validation_y)\n",
    "\n",
    "stat = np.zeros(num_models, dtype=int)\n",
    "for row in target_y:\n",
    "    stat[np.argmax(row)] += 1\n",
    "\n",
    "validation_stats = DataFrame(stat, category_names, ['dim(V_i)'])\n",
    "print(validation_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model from (Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1408.5882]\n",
    "\n",
    "**Notes:**\n",
    "* Don't use L2 norm contraints on weight vectors (see (A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1510.03820]) (info from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow)\n",
    "* the input has 3 filter branches\n",
    "* the fully connected layer has ~~one~~ no hidden layer\n",
    "\n",
    "**Architecture** (from original paper):\n",
    "![Architecture](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:35:32.714391",
     "start_time": "2017-01-21T00:35:32.520691"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of filters of each size\n",
    "num_filters = 128\n",
    "# square filter sizes (3x3, 4x4, and 5x5)\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filter_branches = len(filter_sizes)\n",
    "\n",
    "# add the channel dimension (only 1 channel)\n",
    "# tip with np.expand_dims is from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "validation_X = np.expand_dims(validation_X, -1)\n",
    "\n",
    "# create the filter branches\n",
    "filter_branches = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    branch = Sequential()\n",
    "    branch.add(Convolution2D(num_filters, filter_size, k, init='uniform', border_mode='same',\n",
    "                        input_shape=train_X.shape[1:], W_regularizer=l2(0.01)))\n",
    "    branch.add(Activation('relu'))\n",
    "    pool_size =  num_filters;\n",
    "    branch.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    #branch.add(Dropout(0.25))\n",
    "    filter_branches.append(branch)\n",
    "\n",
    "# merge the branches by concatenating \n",
    "merged_filters = Merge(filter_branches, mode='concat')\n",
    "\n",
    "#create the final model with the filter layers and the fully connected layers\n",
    "model = Sequential()\n",
    "model.add(merged_filters)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_models))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model with an accuracy measurement\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:35:33.646551",
     "start_time": "2017-01-21T00:35:33.641566"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True, write_images=False))\n",
    "callbacks.append(ModelCheckpoint('checkpoints', monitor='acc', verbose=1, save_best_only=True, mode='max'))\n",
    "callbacks.append(EarlyStopping(monitor='loss', min_delta=0.01, patience=3, verbose=1, mode='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:35:39.157931",
     "start_time": "2017-01-21T00:35:39.152190"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# since we have more than one filter branch, tile the input data to all branches\n",
    "# np.tile causes the numpy kernel to crash (probably due to copying of the data. \n",
    "# putting it in a list works, because it then is just referenced 3 times)\n",
    "\n",
    "#tiled_train_X = np.tile(expanded_train_X, (num_filter_branches, 1))\n",
    "#tiled_validation_X = np.tile(expanded_validation_X, (num_filter_branches, 1))\n",
    "\n",
    "tiled_train_X = [train_X, train_X, train_X]\n",
    "tiled_validation_X = [validation_X, validation_X, validation_X]\n",
    "\n",
    "if LEARN:\n",
    "    model.fit(tiled_train_X, train_y, \n",
    "              validation_data=(tiled_validation_X, target_y),\n",
    "              nb_epoch=1, batch_size=128,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:35:42.906892",
     "start_time": "2017-01-21T00:35:41.685043"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if LEARN:\n",
    "    model.save('data/news.cnn.model')\n",
    "else:\n",
    "    from keras.models import load_model\n",
    "    model = load_model('data/news.cnn.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T01:47:20.782056",
     "start_time": "2017-01-21T00:35:47.471333"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_y = model.predict([validation_X, validation_X, validation_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T12:05:54.694581",
     "start_time": "2017-01-21T12:05:54.632344"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: weight the output by the number of elements.\n",
    "# Why? because it may work although it won't make any sense...\n",
    "predicted_y *= max(stat)\n",
    "predicted_y /= stat\n",
    "\n",
    "#convert the log likelyhood prediction to a single-hot vector\n",
    "# with hotspot at index of highest likelihood\n",
    "predicted_y_singlehot = np.zeros(predicted_y.shape)\n",
    "for x, row in enumerate(predicted_y):\n",
    "    max_index = np.where(row == max(row))\n",
    "    predicted_y_singlehot[x, max_index] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T12:05:55.440393",
     "start_time": "2017-01-21T12:05:55.363014"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Aktuell  Ausland  Finanzen  Kultur  Lifestyle  Lokal  Politik  \\\n",
      "Aktuell           20      483       333     208        328    249     1596   \n",
      "Ausland            0        0         0       0          0      0        0   \n",
      "Finanzen           0        0         0       0          0      0        0   \n",
      "Kultur             0        0         0       0          0      0        0   \n",
      "Lifestyle          0        0         0       0          0      0        0   \n",
      "Lokal              0        0         0       0          0      0        0   \n",
      "Politik            0        0         0       0          0      0        0   \n",
      "Sonstiges          0        0         0       0          0      0        0   \n",
      "Sport              0        0         0       0          0      0        0   \n",
      "Technologie        0        0         0       0          0      0        0   \n",
      "Wirtschaft         0        0         0       0          0      0        0   \n",
      "\n",
      "             Sonstiges  Sport  Technologie  Wirtschaft  \n",
      "Aktuell            770    449          361         817  \n",
      "Ausland              0      0            0           0  \n",
      "Finanzen             0      0            0           0  \n",
      "Kultur               0      0            0           0  \n",
      "Lifestyle            0      0            0           0  \n",
      "Lokal                0      0            0           0  \n",
      "Politik              0      0            0           0  \n",
      "Sonstiges            0      0            0           0  \n",
      "Sport                0      0            0           0  \n",
      "Technologie          0      0            0           0  \n",
      "Wirtschaft           0      0            0           0  \n"
     ]
    }
   ],
   "source": [
    "classification_matrix = np.zeros([num_models, num_models], dtype=int)\n",
    "\n",
    "for i in range(len(predicted_y_singlehot)):\n",
    "    predicted_index = np.where(predicted_y_singlehot[i] == 1)[0]\n",
    "    target_index = np.where(target_y[i] == 1)[0]\n",
    "    classification_matrix[predicted_index, target_index] += 1\n",
    "\n",
    "result = DataFrame(classification_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
