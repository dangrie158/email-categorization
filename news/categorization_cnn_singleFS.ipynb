{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a CNN for news topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:52:30.983093",
     "start_time": "2017-01-16T16:52:29.446707"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:53:15.972546",
     "start_time": "2017-01-16T16:52:31.573635"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel has 200 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "category_names = ['Sonstiges', 'Aktuell', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.200dim.word2vec.model')\n",
    "\n",
    "# we can precompute the L2-normalized vectors to save lots of memory\n",
    "# we can't continue learning after they are normalized but the model is static\n",
    "# in this usecase anyways\n",
    "base_model.init_sims(replace=True)\n",
    "\n",
    "k = base_model.vector_size\n",
    "print(\"basemodel has {} dimensional vectors\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** stopword filtering is commented out, check if filtering stopwords improves perfomance, but since the cnn learns \"patterns\" the filtering may distort the pattern too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:53:22.786670",
     "start_time": "2017-01-16T16:53:22.774669"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split()]# if x not in stopwords]\n",
    "                X.append(tokens)\n",
    "                y.append(name)\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-25T11:26:15.096608",
     "start_time": "2016-12-25T11:21:21.000650"
    },
    "collapsed": true
   },
   "source": [
    "load the raw train and validation datasets in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:53:25.556297",
     "start_time": "2017-01-16T16:53:24.254613"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 28132 articles\n",
      "loaded 12042 articles\n"
     ]
    }
   ],
   "source": [
    "raw_train_X, raw_train_y = load_sets(train_paths)\n",
    "raw_validation_X, raw_validation_y = load_sets(validation_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the average article length $N$\n",
    "\n",
    "$$N_{avg}=\\frac { \\sum _{ x\\in X }^{  }{ dim(x) }  }{ dim(X) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:53:27.544144",
     "start_time": "2017-01-16T16:53:27.533275"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average article length is: 223 words\n"
     ]
    }
   ],
   "source": [
    "N_avg = 0\n",
    "N_avg = sum([len(article) for article in raw_train_X]) / len(raw_train_X)\n",
    "print(\"average article length is: {} words\".format(N_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-31T10:43:14.290958",
     "start_time": "2016-12-31T10:43:14.286344"
    }
   },
   "source": [
    "convert the raw input article to matrices of word-vectors.\n",
    "each article $x$ is represented as\n",
    "\n",
    "$${ x }_{ 1:n }={ x }_{ 1 }\\oplus { x }_{ 2 }\\oplus \\dots \\oplus { x }_{ n }$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ i }\\in { { R } }^{ k }$ is the $k$-dimensional word-embedding vector for the $i$-th word in the article.\n",
    "* $\\oplus$ is the concatenation operator\n",
    "\n",
    "the result is a matrix for each article in the form:\n",
    "$$x=\\begin{bmatrix} { x }_{ 1,1 } & { x }_{ 1,2 } & \\cdots  & { x }_{ 1,k } \\\\ { x }_{ 2,1 } & { x }_{ 2,2 } & \\cdots  & { x }_{ 2,k } \\\\ \\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ { x }_{ n,1 } & { x }_{ n,2 } & \\cdots  & { x }_{ n,k } \\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ n,k }$ is the value of the $k$-th dimension of the word-vector for word $n$\n",
    "\n",
    "the matrix is padded or cropped to a length of $N_{avg}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:54:00.898644",
     "start_time": "2017-01-16T16:53:29.262410"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def articles_to_matrices(articles, word_dim, article_len):\n",
    "    X = np.zeros((len(articles), article_len, word_dim), dtype='float32')\n",
    "    for x, raw_article in enumerate(articles):\n",
    "        for x_n in range(article_len):\n",
    "            # if the word can't be found, use zero vector\n",
    "            word_vec = np.zeros(word_dim)\n",
    "            # try to load the word from the basemodel\n",
    "            # TODO: maybe skip non-available words rather than default-zero\n",
    "            # so if more then N_avg words ar in the article they get used\n",
    "            try:\n",
    "                word_vec = base_model[raw_article[x_n]]\n",
    "            except:\n",
    "                pass\n",
    "            X[x, x_n] = word_vec\n",
    "    return X\n",
    "\n",
    "train_X = articles_to_matrices(raw_train_X, k, N_avg)\n",
    "validation_X = articles_to_matrices(raw_validation_X, k, N_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the string train input data to a one-hot vector that can be used on the output layer of the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:54:06.081074",
     "start_time": "2017-01-16T16:54:06.021895"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categories_to_one_hot(categories):\n",
    "    _, int_y = np.unique(categories, return_inverse=True)\n",
    "    y = np_utils.to_categorical(int_y)\n",
    "    return y\n",
    "\n",
    "train_y = categories_to_one_hot(raw_train_y)\n",
    "target_y = categories_to_one_hot(raw_validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model from (Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1408.5882]\n",
    "\n",
    "**Notes:**\n",
    "* Don't use L2 norm contraints on weight vectors (see (A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1510.03820]) (info from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow)\n",
    "* the input has 3 filter branches\n",
    "* the fully connected layer has ~~one~~ no hidden layer\n",
    "\n",
    "**Architecture** (from original paper):\n",
    "![Architecture](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:54:07.874853",
     "start_time": "2017-01-16T16:54:07.595577"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of filters of each size\n",
    "num_filters = 128\n",
    "# square filter sizes (3x3, 4x4, and 5x5)\n",
    "filter_sizes = [3, 4, 5]\n",
    "filter_size = 12\n",
    "num_filter_branches = len(filter_sizes)\n",
    "\n",
    "# add the channel dimension (only 1 channel)\n",
    "# tip with np.expand_dims is from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "validation_X = np.expand_dims(validation_X, -1)\n",
    "\n",
    "# create the filter branches\n",
    "#filter_branches = []\n",
    "#for i, filter_size in enumerate(filter_sizes):\n",
    "branch = Sequential()\n",
    "branch.add(Convolution2D(num_filters, filter_size, k, init='uniform', border_mode='same',\n",
    "                        input_shape=train_X.shape[1:], W_regularizer=l2(0.01)))\n",
    "branch.add(Activation('relu'))\n",
    "pool_size =  num_filters;\n",
    "branch.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "#branch.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "# merge the branches by concatenating \n",
    "#%%!merged_filters = Merge(filter_branches, mode='concat')\n",
    "\n",
    "#create the final model with the filter layers and the fully connected layers\n",
    "model = Sequential()\n",
    "model.add(branch)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_models))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model with an accuracy measurement\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-16T16:54:10.118669",
     "start_time": "2017-01-16T16:54:10.112743"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True, write_images=True))\n",
    "callbacks.append(ModelCheckpoint('checkpoints', monitor='val_acc', verbose=1, save_best_only=False, mode='max'))\n",
    "callbacks.append(EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3, verbose=1, mode='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-16T15:54:48.457Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28132 samples, validate on 12042 samples\n",
      "Epoch 1/1\n",
      "27904/28132 [============================>.] - ETA: 587s - loss: 3.4348 - acc: 0.2602 "
     ]
    }
   ],
   "source": [
    "# since we have more than one filter branch, tile the input data to all branches\n",
    "# np.tile causes the numpy kernel to crash (probably due to copying of the data. \n",
    "# putting it in a list works, because it then is just referenced 3 times)\n",
    "\n",
    "#tiled_train_X = np.tile(expanded_train_X, (num_filter_branches, 1))\n",
    "#tiled_validation_X = np.tile(expanded_validation_X, (num_filter_branches, 1))\n",
    "\n",
    "#tiled_train_X = [train_X, train_X, train_X]\n",
    "#tiled_validation_X = [validation_X, validation_X, validation_X]\n",
    "\n",
    "model.fit(train_X, train_y, \n",
    "          validation_data=(validation_X, target_y),\n",
    "          nb_epoch=1, batch_size=256,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('data/news.cnn.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
