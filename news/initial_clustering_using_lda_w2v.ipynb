{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:26:12.894435",
     "start_time": "2017-01-30T14:26:12.723362"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.cm as cm\n",
    "from nltk.corpus import stopwords as sw\n",
    "#from nltk.stem.snowball import GermanStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from sklearn import cluster\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from scipy import spatial\n",
    "from sklearn import preprocessing\n",
    "from pandas import DataFrame\n",
    "import pickle\n",
    "\n",
    "\n",
    "# there are quite a few long-running processes in this notebook.\n",
    "# activated logging is a good way to get a status of these tasks\n",
    "# disable logging for \"presentation notebooks\" since the logging uses stderr\n",
    "import logging\n",
    "\n",
    "stopwords = sw.words('german')\n",
    "# gensims LineSentence generator replaces umlauts with \n",
    "# u, a or o so add these variants to the stopwordlist\n",
    "for stopword in stopwords:\n",
    "    stopword = stopword.replace(u'ü', 'u')\n",
    "    stopword = stopword.replace(u'ö', 'o')\n",
    "    stopword = stopword.replace(u'ä', 'a')\n",
    "    if stopword not in stopwords:\n",
    "        stopwords.append(stopword)\n",
    "        \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:26:59.316129",
     "start_time": "2017-01-30T14:26:13.454747"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel has 200 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "category_names = ['Sonstiges', 'Aktuell', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.200dim.word2vec.model')\n",
    "\n",
    "# we can precompute the L2-normalized vectors to save lots of memory\n",
    "# we can't continue learning after they are normalized but the model is static\n",
    "# in this usecase anyways\n",
    "base_model.init_sims(replace=True)\n",
    "\n",
    "k = base_model.vector_size\n",
    "print(\"basemodel has {} dimensional vectors\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:27:02.299888",
     "start_time": "2017-01-30T14:27:02.284162"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    log = logging.getLogger('load-sets')\n",
    "    #stemmer = GermanStemmer()\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            log.info('now loading path {} ...'.format(path))\n",
    "            \n",
    "            #get the number of lines for logging and rewind the file to start\n",
    "            lines = sum([1 for line in cur_file])\n",
    "            cur_file.seek(0)\n",
    "            \n",
    "            for num, line in enumerate(cur_file):\n",
    "                if num % 1000 == 0:\n",
    "                    log.info('preprocessed {} of {} lines'.format(num, lines))\n",
    "                tokens = [x for x in line.split() if x not in stopwords]\n",
    "                if len(tokens) > 0:\n",
    "                    X.append(tokens)\n",
    "                    y.append(name)\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:32:03.040518",
     "start_time": "2017-01-30T14:27:03.237723"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:17: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 32200 articles\n"
     ]
    }
   ],
   "source": [
    "X, target = load_sets(fulldata_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:32:18.269033",
     "start_time": "2017-01-30T14:32:03.042951"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:32:31.534702",
     "start_time": "2017-01-30T14:32:18.270839"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:32:32.571315",
     "start_time": "2017-01-30T14:32:31.536264"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel_filename = 'data/corpus.lda.model'\n",
    "try:\n",
    "    ldamodel = LdaModel.load(ldamodel_filename)\n",
    "except IOError:\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    ldamodel = LdaModel(corpus, num_topics=100, id2word = dictionary, passes=20, random_state=0)\n",
    "    logging.disable(logging.CRITICAL);\n",
    "    ldamodel.save(ldamodel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:32:48.740823",
     "start_time": "2017-01-30T14:32:32.572854"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.054*\"apple\" + 0.035*\"android\" + 0.033*\"smartphone\" + 0.027*\"iphone\" + 0.023*\"windows\"'),\n",
       " (1,\n",
       "  u'0.073*\"berlin\" + 0.069*\"prosieben\" + 0.065*\"\\u201eanne\" + 0.059*\"will\\u201c\" + 0.039*\"xy\"'),\n",
       " (2,\n",
       "  u'0.036*\"turkei\" + 0.021*\"erdogan\" + 0.018*\"regierung\" + 0.014*\"turkische\" + 0.013*\"parlament\"'),\n",
       " (3,\n",
       "  u'0.036*\"klum\" + 0.032*\"haushalts\" + 0.032*\"microsoft\" + 0.022*\"\\u201esex\" + 0.015*\"reality\"'),\n",
       " (4,\n",
       "  u'0.014*\"auto\" + 0.012*\"autos\" + 0.011*\"neue\" + 0.010*\"hersteller\" + 0.009*\"bmw\"'),\n",
       " (5,\n",
       "  u'0.048*\"\\u201eard\" + 0.022*\"golem\" + 0.019*\"intelligenz\" + 0.018*\"roboter\" + 0.014*\"boni\"'),\n",
       " (6,\n",
       "  u'0.022*\"stinkt\" + 0.021*\"ruckrunde\" + 0.021*\"\\u201estar\" + 0.020*\"\\u201evom\" + 0.015*\"depression\"'),\n",
       " (7,\n",
       "  u'0.053*\"sommerhaus\" + 0.046*\"dfb\" + 0.042*\"mehr\\u201c\" + 0.032*\"frankfurt\" + 0.020*\"park\"'),\n",
       " (8,\n",
       "  u'0.096*\"eu\" + 0.020*\"deutschland\" + 0.018*\"kommission\" + 0.018*\"europa\" + 0.017*\"europaischen\"'),\n",
       " (9,\n",
       "  u'0.102*\"spiegel\" + 0.083*\"online\" + 0.023*\"rohe\" + 0.019*\"modus\" + 0.018*\"modern\"'),\n",
       " (10,\n",
       "  u'0.038*\"united\" + 0.029*\"rheinland\" + 0.025*\"pfalz\" + 0.025*\"bonn\" + 0.022*\"leipzig\"'),\n",
       " (11,\n",
       "  u'0.015*\"versucht\" + 0.010*\"lewis\" + 0.010*\"shopping\" + 0.010*\"tierischen\" + 0.007*\"ecke\"'),\n",
       " (12,\n",
       "  u'0.073*\"hamilton\" + 0.034*\"stellenmarkt\" + 0.016*\"becker\" + 0.012*\"djokovic\" + 0.011*\"aktiviert\"'),\n",
       " (13,\n",
       "  u'0.054*\"brexit\" + 0.036*\"britische\" + 0.035*\"eu\" + 0.032*\"gro\\xdfbritannien\" + 0.029*\"may\"'),\n",
       " (14,\n",
       "  u'0.044*\"\\u201ezdf\" + 0.036*\"sacher\" + 0.031*\"fleisch\" + 0.027*\"bauer\" + 0.023*\"schmidt\"'),\n",
       " (15,\n",
       "  u'0.014*\"schon\" + 0.008*\"mehr\" + 0.008*\"mal\" + 0.007*\"beim\" + 0.006*\"zwei\"'),\n",
       " (16,\n",
       "  u'0.038*\"warrior\" + 0.028*\"ferrer\" + 0.026*\"love\\u201c\" + 0.024*\"weiss\" + 0.014*\"wirres\"'),\n",
       " (17,\n",
       "  u'0.022*\"lauda\" + 0.019*\"weltmeisterschaft\" + 0.017*\"weltall\" + 0.017*\"mclaren\" + 0.017*\"punk\"'),\n",
       " (18,\n",
       "  u'0.044*\"nutzer\" + 0.025*\"spielplan\" + 0.023*\"ndr\" + 0.016*\"video\" + 0.016*\"gericht\"'),\n",
       " (19,\n",
       "  u'0.022*\"\\u201edeutschlands\" + 0.022*\"ubersicht\" + 0.018*\"steinbruck\" + 0.015*\"talkrunde\" + 0.015*\"grill\"'),\n",
       " (20,\n",
       "  u'0.032*\"hamburg\" + 0.026*\"ferrari\" + 0.024*\"unfall\" + 0.018*\"sachsen\" + 0.016*\"fahrer\"'),\n",
       " (21,\n",
       "  u'0.035*\"onlinekauf\" + 0.034*\"amazon\" + 0.027*\"vox\" + 0.025*\"s\" + 0.025*\"germany\\u201c\"'),\n",
       " (22,\n",
       "  u'0.025*\"drohne\" + 0.022*\"tel\" + 0.020*\"aviv\" + 0.019*\"rakete\" + 0.017*\"\\u201efocus\"'),\n",
       " (23,\n",
       "  u'0.073*\"mond\" + 0.024*\"oettinger\" + 0.021*\"wlan\" + 0.020*\"cernan\" + 0.016*\"burka\"'),\n",
       " (24,\n",
       "  u'0.017*\"2016\\u201c\" + 0.017*\"2016\" + 0.016*\"ausgabe\" + 0.012*\"2017\" + 0.012*\"auktion\"'),\n",
       " (25,\n",
       "  u'0.074*\"doku\" + 0.049*\"experiment\" + 0.027*\"til\" + 0.021*\"jauchs\" + 0.017*\"katzenberger\"'),\n",
       " (26,\n",
       "  u'0.013*\"leben\\u201c\" + 0.013*\"eva\" + 0.013*\"tatort\" + 0.009*\"\\u201etatort\\u201c\" + 0.008*\"jahren\"'),\n",
       " (27,\n",
       "  u'0.025*\"dusseldorf\" + 0.016*\"hamster\" + 0.016*\"basler\" + 0.013*\"pilot\" + 0.011*\"glanzen\"'),\n",
       " (28,\n",
       "  u'0.038*\"\\xbb\" + 0.024*\"artikel\" + 0.022*\"fehler\" + 0.021*\"kaufen\" + 0.020*\"focus\"'),\n",
       " (29,\n",
       "  u'0.049*\"kommentator\" + 0.044*\"uli\" + 0.030*\"play\" + 0.026*\"neckar\" + 0.025*\"gratis\"'),\n",
       " (30,\n",
       "  u'0.043*\"sarah\" + 0.030*\"\\u201edeutschland\" + 0.025*\"bohlen\" + 0.024*\"pietro\" + 0.023*\"dsds\"'),\n",
       " (31,\n",
       "  u'0.044*\"hoene\\xdf\" + 0.018*\"klopp\" + 0.018*\"liverpool\" + 0.017*\"pokal\" + 0.014*\"hummels\"'),\n",
       " (32,\n",
       "  u'0.040*\"illner\" + 0.028*\"maybrit\" + 0.017*\"stars\" + 0.016*\"dance\" + 0.015*\"ex\"'),\n",
       " (33,\n",
       "  u'0.062*\"maischberger\" + 0.034*\"\\u201eaktenzeichen\" + 0.030*\"bachelorette\" + 0.019*\"wagenknecht\" + 0.018*\"\\u201eberlin\"'),\n",
       " (34,\n",
       "  u'0.036*\"1\" + 0.023*\"formel\" + 0.014*\"em\" + 0.013*\"wm\" + 0.013*\"mercedes\"'),\n",
       " (35,\n",
       "  u'0.020*\"wissen\" + 0.020*\"mehr\" + 0.019*\"mail\" + 0.017*\"n24\" + 0.014*\"nachrichten\"'),\n",
       " (36,\n",
       "  u'0.070*\"fifa\" + 0.035*\"wm\" + 0.026*\"kahn\" + 0.019*\"schweighofer\" + 0.019*\"brangelina\"'),\n",
       " (37,\n",
       "  u'0.022*\"dating\" + 0.021*\"fisch\" + 0.020*\"malzer\" + 0.016*\"display\" + 0.015*\"jurgens\"'),\n",
       " (38,\n",
       "  u'0.152*\"\\u201ehart\" + 0.135*\"fair\\u201c\" + 0.023*\"vettel\" + 0.023*\"rot\" + 0.020*\"welt\\u201c\"'),\n",
       " (39,\n",
       "  u'0.053*\"nsu\" + 0.037*\"verfassungsschutz\" + 0.028*\"hubert\" + 0.024*\"verkneifen\" + 0.024*\"v\"'),\n",
       " (40,\n",
       "  u'0.058*\"fu\\xdfball\" + 0.044*\"bayern\" + 0.034*\"fc\" + 0.029*\"bundesliga\" + 0.021*\"munchen\"'),\n",
       " (41,\n",
       "  u'0.054*\"merkel\" + 0.042*\"cdu\" + 0.038*\"spd\" + 0.018*\"angela\" + 0.017*\"csu\"'),\n",
       " (42,\n",
       "  u'0.028*\"sigurdsson\" + 0.026*\"turnier\" + 0.024*\"bundestrainer\" + 0.020*\"sahra\" + 0.019*\"deutschen\"'),\n",
       " (43,\n",
       "  u'0.031*\"polizei\" + 0.017*\"mann\" + 0.015*\"worden\" + 0.011*\"frau\" + 0.010*\"wegen\"'),\n",
       " (44,\n",
       "  u'0.033*\"facebook\" + 0.014*\"internet\" + 0.012*\"daten\" + 0.011*\"informationen\" + 0.010*\"fake\"'),\n",
       " (45,\n",
       "  u'0.045*\"ronaldo\" + 0.043*\"deckt\" + 0.033*\"spezial\" + 0.028*\"grad\" + 0.024*\"liveticker\"'),\n",
       " (46,\n",
       "  u'0.009*\"leben\" + 0.009*\"dass\" + 0.009*\"mal\" + 0.009*\"dschungel\" + 0.009*\"frau\"'),\n",
       " (47,\n",
       "  u'0.028*\"atzt\" + 0.023*\"tuv\" + 0.018*\"psychiater\" + 0.016*\"bahnt\" + 0.015*\"xxl\"'),\n",
       " (48,\n",
       "  u'0.052*\"dschungelcamp\" + 0.036*\"rtl\" + 0.021*\"star\" + 0.019*\"staffel\" + 0.017*\"doku\"'),\n",
       " (49,\n",
       "  u'0.033*\"telekom\" + 0.026*\"\\u201echristopher\" + 0.019*\"hacker\" + 0.017*\"partien\" + 0.017*\"contest\"'),\n",
       " (50,\n",
       "  u'0.284*\"kolumne\" + 0.038*\"\\u2013\" + 0.029*\"ard\" + 0.024*\"\\u201c\" + 0.019*\"kritik\"'),\n",
       " (51,\n",
       "  u'0.090*\"lanz\" + 0.082*\"jauch\" + 0.039*\"gunther\" + 0.018*\"fernsehen\" + 0.017*\"big\"'),\n",
       " (52,\n",
       "  u'0.096*\"koln\" + 0.033*\"kolner\" + 0.032*\"wolfratshausen\" + 0.032*\"freising\" + 0.032*\"ebersberg\"'),\n",
       " (53,\n",
       "  u'0.021*\"leverkusen\" + 0.017*\"asteroid\" + 0.015*\"knallhart\" + 0.015*\"tier\" + 0.011*\"enttarnt\"'),\n",
       " (54,\n",
       "  u'0.026*\"klub\" + 0.020*\"konter\" + 0.015*\"welpen\" + 0.015*\"ingolstadt\" + 0.014*\"feuerloscher\"'),\n",
       " (55,\n",
       "  u'0.011*\"wer\" + 0.011*\"sollten\" + 0.010*\"app\" + 0.008*\"beim\" + 0.008*\"nutzer\"'),\n",
       " (56,\n",
       "  u'0.019*\"anbieter\" + 0.019*\"sieger\" + 0.019*\"b\" + 0.018*\"highlights\" + 0.017*\"achtelfinale\"'),\n",
       " (57,\n",
       "  u'0.047*\"\\u201ewie\" + 0.032*\"terenzi\" + 0.020*\"barth\" + 0.015*\"sherlock\" + 0.014*\"zuschauen\"'),\n",
       " (58, u'0.037*\"1\" + 0.030*\"uhr\" + 0.026*\"0\" + 0.020*\"live\" + 0.020*\"2\"'),\n",
       " (59,\n",
       "  u'0.035*\"menschen\" + 0.021*\"worden\" + 0.016*\"seien\" + 0.011*\"sagte\" + 0.011*\"angaben\"'),\n",
       " (60,\n",
       "  u'0.025*\"real\" + 0.021*\"barcelona\" + 0.017*\"madrid\" + 0.015*\"geiseln\" + 0.013*\"boateng\"'),\n",
       " (61,\n",
       "  u'0.054*\"sz\" + 0.035*\"zeitung\" + 0.029*\"de\" + 0.027*\"anzeigen\" + 0.022*\"munchen\"'),\n",
       " (62,\n",
       "  u'0.018*\"hohepunkte\" + 0.017*\"prinz\" + 0.015*\"elizabeth\" + 0.015*\"angreifen\" + 0.013*\"wales\"'),\n",
       " (63,\n",
       "  u'0.068*\"dance\\u201c\" + 0.025*\"west\" + 0.025*\"kim\" + 0.021*\"kardashian\" + 0.018*\"kanye\"'),\n",
       " (64,\n",
       "  u'0.019*\"unternehmen\" + 0.018*\"us\" + 0.017*\"dollar\" + 0.012*\"wirtschaft\" + 0.011*\"milliarden\"'),\n",
       " (65,\n",
       "  u'0.030*\"erde\" + 0.026*\"forscher\" + 0.016*\"nasa\" + 0.013*\"all\" + 0.013*\"stream\"'),\n",
       " (66,\n",
       "  u'0.008*\"mussen\" + 0.008*\"gesetz\" + 0.008*\"patienten\" + 0.007*\"wegen\" + 0.006*\"bundesregierung\"'),\n",
       " (67,\n",
       "  u'0.031*\"dass\" + 0.009*\"mehr\" + 0.007*\"gibt\" + 0.007*\"immer\" + 0.006*\"\\u2013\"'),\n",
       " (68,\n",
       "  u'0.032*\"mehr\" + 0.031*\"beckmann\" + 0.029*\"partnersuche\" + 0.027*\"zeit\" + 0.023*\"immobilien\"'),\n",
       " (69,\n",
       "  u'0.051*\"the\" + 0.044*\"of\" + 0.015*\"film\" + 0.012*\"millionar\" + 0.009*\"to\"'),\n",
       " (70,\n",
       "  u'0.056*\"frauen\" + 0.041*\"\\u201epromi\" + 0.033*\"brother\\u201c\" + 0.020*\"manner\" + 0.018*\"gewalt\"'),\n",
       " (71,\n",
       "  u'0.133*\"jauch\\u201c\" + 0.093*\"fair\" + 0.019*\"billig\" + 0.010*\"nervt\" + 0.010*\"zoff\"'),\n",
       " (72,\n",
       "  u'0.066*\"afd\" + 0.018*\"petry\" + 0.018*\"partei\" + 0.014*\"hocke\" + 0.013*\"grad\\u201c\"'),\n",
       " (73,\n",
       "  u'0.029*\"schwiegertochter\" + 0.026*\"irene\" + 0.019*\"deutschland\" + 0.019*\"beate\" + 0.017*\"gesucht\"'),\n",
       " (74,\n",
       "  u'0.022*\"deutschland\" + 0.013*\"uberschattet\" + 0.013*\"giulia\" + 0.012*\"heels\" + 0.011*\"klassenfahrt\"'),\n",
       " (75,\n",
       "  u'0.093*\"trump\" + 0.050*\"us\" + 0.027*\"donald\" + 0.023*\"usa\" + 0.022*\"prasident\"'),\n",
       " (76,\n",
       "  u'0.032*\"russland\" + 0.013*\"putin\" + 0.011*\"russische\" + 0.010*\"van\" + 0.010*\"russischen\"'),\n",
       " (77,\n",
       "  u'0.053*\"is\" + 0.031*\"syrien\" + 0.023*\"aleppo\" + 0.013*\"anschlag\" + 0.011*\"al\"'),\n",
       " (78,\n",
       "  u'0.035*\"sagte\" + 0.021*\"sei\" + 0.020*\"dass\" + 0.011*\"bereits\" + 0.009*\"worden\"'),\n",
       " (79,\n",
       "  u'0.026*\"\\u201epolizeiruf\" + 0.024*\"israel\" + 0.015*\"mandalas\" + 0.015*\"gesundheits\" + 0.013*\"110\\u201c\"'),\n",
       " (80,\n",
       "  u'0.025*\"us\" + 0.017*\"japan\" + 0.014*\"dollar\" + 0.014*\"tillerson\" + 0.014*\"schritt\\u201c\"'),\n",
       " (81,\n",
       "  u'0.086*\"vw\" + 0.032*\"volkswagen\" + 0.028*\"diesel\" + 0.022*\"\\u201emario\" + 0.018*\"us\"'),\n",
       " (82,\n",
       "  u'0.067*\"china\" + 0.031*\"jones\" + 0.020*\"\\u201eberliner\" + 0.020*\"chinesische\" + 0.019*\"chinesischen\"'),\n",
       " (83,\n",
       "  u'0.049*\"italien\" + 0.037*\"gottschalk\" + 0.020*\"renzi\" + 0.017*\"referendum\" + 0.011*\"rucktritt\"'),\n",
       " (84,\n",
       "  u'0.054*\"partnerangebot\" + 0.038*\"\\u201elet\" + 0.025*\"altmaier\" + 0.024*\"super\" + 0.020*\"bang\"'),\n",
       " (85,\n",
       "  u'0.403*\"tv\" + 0.023*\"zdf\" + 0.021*\"\\u201egunther\" + 0.013*\"hart\" + 0.010*\"show\"'),\n",
       " (86,\n",
       "  u'0.040*\"l\" + 0.039*\"maria\" + 0.034*\"freiburg\" + 0.023*\"knast\" + 0.021*\"\\u201eechte\"'),\n",
       " (87,\n",
       "  u'0.043*\"dresden\" + 0.023*\"kirche\" + 0.022*\"christen\" + 0.016*\"wolfe\" + 0.016*\"galilei\"'),\n",
       " (88,\n",
       "  u'0.070*\"gina\" + 0.067*\"camp\" + 0.046*\"vip\" + 0.038*\"marc\" + 0.030*\"markus\"'),\n",
       " (89,\n",
       "  u'0.078*\"wetter\" + 0.042*\"samt\" + 0.042*\"tage\" + 0.042*\"nachsten\" + 0.041*\"bekommen\"'),\n",
       " (90,\n",
       "  u'0.111*\"check\" + 0.077*\"bachelor\" + 0.027*\"report\" + 0.021*\"freiberg\" + 0.014*\"universitat\"'),\n",
       " (91,\n",
       "  u'0.026*\"kabelsender\" + 0.022*\"mausklick\" + 0.019*\"streaming\" + 0.017*\"halbfinal\" + 0.016*\"schaulaufen\"'),\n",
       " (92,\n",
       "  u'0.032*\"wallraff\" + 0.014*\"mafia\" + 0.013*\"\\u201eadam\" + 0.013*\"eva\\u201c\" + 0.012*\"pocher\"'),\n",
       " (93,\n",
       "  u'0.037*\"3sat\" + 0.028*\"npd\" + 0.014*\"storys\" + 0.013*\"verstort\" + 0.012*\"buhlen\"'),\n",
       " (94,\n",
       "  u'0.047*\"baden\" + 0.036*\"wurttemberg\" + 0.024*\"kerber\" + 0.021*\"erdbeben\" + 0.019*\"reinigen\"'),\n",
       " (95,\n",
       "  u'0.032*\"frankreich\" + 0.027*\"clips\" + 0.025*\"immobilienmarkt\" + 0.022*\"le\" + 0.020*\"backer\"'),\n",
       " (96,\n",
       "  u'0.050*\"bank\" + 0.031*\"ungarn\" + 0.031*\"gysi\" + 0.024*\"iv\" + 0.023*\"hartz\"'),\n",
       " (97,\n",
       "  u'0.030*\"fluchtlinge\" + 0.018*\"deutschland\" + 0.016*\"grad\" + 0.013*\"mehr\" + 0.009*\"000\"'),\n",
       " (98,\n",
       "  u'0.030*\"bull\" + 0.028*\"red\" + 0.015*\"darmstadt\" + 0.015*\"versorger\" + 0.014*\"monaco\"'),\n",
       " (99,\n",
       "  u'0.031*\"prozent\" + 0.023*\"euro\" + 0.016*\"millionen\" + 0.016*\"mehr\" + 0.015*\"jahr\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=100, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:33:04.951061",
     "start_time": "2017-01-30T14:32:48.743065"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#precompute the top words from the LDA model\n",
    "WORDS_PER_TOPIC = 10\n",
    "topwords_in_topic = np.zeros([100, WORDS_PER_TOPIC, k])\n",
    "for topic in range(100):\n",
    "    words_added = 0\n",
    "    for top_word_id, probability in ldamodel.get_topic_terms(topic, topn=100):\n",
    "        top_word = dictionary.get(top_word_id)\n",
    "        try:\n",
    "            top_word_vec = base_model[top_word]\n",
    "            topwords_in_topic[topic, words_added] = top_word_vec\n",
    "            words_added += 1\n",
    "            if words_added == WORDS_PER_TOPIC:\n",
    "                break\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the (cosine) similarity \n",
    "$$cos(\\theta )=\\frac { { w }_{ d,i }\\cdot { v }_{ t,j } }{ \\left| { w }_{ d,i } \\right| *\\left| { v }_{ t,j } \\right|  } $$\n",
    "\n",
    "of each word of article $d$ with each of the top 5 words ${v}_{t,j}$ with $j\\in[0..4]$ of each topic $t$\n",
    "\n",
    "find the best matching topic\n",
    "\n",
    "$$ \\underset { { v }_{ t,j } }{ argmax } \\ cos(\\theta ) $$ \n",
    "\n",
    "for each word ${ w }_{ d,i }$\n",
    "\n",
    "```X_by_topic``` is a list of dictionaries where each dictionary maps $$ t \\rightarrow \\sum _{ t\\in T }^{  }{ \\begin{cases} 1\\ if\\ \\underset { { v }_{ t,j } }{ argmax } \\ cos(\\theta )\\in T \\\\ 0\\ otherwise \\end{cases} } $$\n",
    "which is the count for how many words of the article the topic $T$ is the best match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:33:13.827673",
     "start_time": "2017-01-30T14:33:04.952942"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping_filename = 'data/corpus.lda.mapping'\n",
    "# try to load the cached file\n",
    "try:\n",
    "    X_by_topic = pickle.load(open(mapping_filename, 'rb'))\n",
    "except IOError:\n",
    "    #if there is no previously saved file, create a new mapping\n",
    "\n",
    "    def get_word(word):\n",
    "        try:\n",
    "            return base_model[word]\n",
    "        except:\n",
    "            return base_model.seeded_vector(word)\n",
    "\n",
    "    X_by_topic = []\n",
    "    for i, article in enumerate(X):\n",
    "        words_vecs = np.asarray([get_word(word) for word in article], dtype=float)\n",
    "        best_topics_for_words = [-1 for word in article]\n",
    "        best_similarities_for_words = [-1 for word in article]\n",
    "        for topic in range(100):\n",
    "            topic_vecs = topwords_in_topic[topic]\n",
    "\n",
    "            # consine calculation of a matrix\n",
    "            # code adapted from http://stackoverflow.com/questions/17627219/whats-the-fastest-way-in-python-to-calculate-cosine-similarity-given-sparse-mat\n",
    "            similarity = np.dot(words_vecs, topic_vecs.T)\n",
    "\n",
    "            norm_words = np.linalg.norm(words_vecs, axis=1)\n",
    "            norm_topics = np.linalg.norm(topic_vecs, axis=1)\n",
    "\n",
    "            # there should be a way to \n",
    "            for word_num in range(similarity.shape[0]):\n",
    "                for j in range(similarity.shape[1]):\n",
    "                    similarity[word_num,j] /= norm_words[word_num] * norm_topics[j]\n",
    "\n",
    "\n",
    "            best_word_indices_in_topic = np.argmax(similarity, axis=1)\n",
    "\n",
    "            for word_num in range(words_vecs.shape[0]):\n",
    "                new_similarity = similarity[word_num, best_word_indices_in_topic[word_num]]\n",
    "                if new_similarity > best_similarities_for_words[word_num]:\n",
    "                    best_similarities_for_words[word_num] = new_similarity\n",
    "                    best_topics_for_words[word_num] = topic\n",
    "\n",
    "        X_by_topic.append(Counter(best_topics_for_words))\n",
    "\n",
    "        if i%100==0:\n",
    "            print(i)  \n",
    "\n",
    "    #save the mapping data\n",
    "    pickle.dump(X_by_topic, open(mapping_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:42:02.912121",
     "start_time": "2017-01-30T14:42:01.857589"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_for_article = []\n",
    "MIN_CONFIDENCE_THRESHOLD = 0.00\n",
    "for art in X_by_topic:\n",
    "    max_category = max(art.iterkeys(), key=lambda x: art[x])\n",
    "    sorted_number_in_category = sorted(art.itervalues(), reverse=True)\n",
    "    max_category_value = float(sorted_number_in_category[0] if len(art) > 0 else 0)\n",
    "    second_max_category_value = float(sorted_number_in_category[1] if len(art) > 1 else 0)\n",
    "    \n",
    "    max_category_value /= sum(sorted_number_in_category)\n",
    "    second_max_category_value /= sum(sorted_number_in_category)\n",
    "    \n",
    "    #print(max_category_value - second_max_category_value)\n",
    "    \n",
    "    if max_category_value - second_max_category_value > MIN_CONFIDENCE_THRESHOLD:\n",
    "        topic_for_article.append(max_category)\n",
    "    else:\n",
    "        topic_for_article.append(100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:42:02.988534",
     "start_time": "2017-01-30T14:42:02.913588"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Aktuell  Ausland  Finanzen  Kultur  Lifestyle  Lokal  Politik  Sonstiges  \\\n",
      "0        0.0      0.0       3.0     1.0        1.0    1.0      0.0        0.0   \n",
      "1        0.0      7.0       8.0     2.0       11.0   52.0     40.0       34.0   \n",
      "2        6.0    199.0       8.0     4.0       14.0    7.0    374.0       55.0   \n",
      "3        0.0      0.0       1.0     1.0        1.0    2.0      1.0        4.0   \n",
      "4        3.0      6.0      29.0     4.0       20.0   13.0     13.0       56.0   \n",
      "5        0.0      0.0       0.0     0.0        0.0    1.0      2.0        1.0   \n",
      "6        0.0      0.0       1.0     0.0        8.0    1.0      0.0        4.0   \n",
      "7        1.0     13.0       3.0     0.0        7.0    8.0     30.0        3.0   \n",
      "8        3.0    112.0      46.0     8.0       12.0   10.0    266.0       25.0   \n",
      "9        0.0      2.0       1.0     7.0        2.0    0.0      3.0        9.0   \n",
      "10       0.0      0.0       0.0     0.0        1.0   38.0      5.0        6.0   \n",
      "11       0.0      3.0       0.0     3.0        8.0    1.0      4.0       21.0   \n",
      "12       0.0      1.0       0.0     3.0        1.0    2.0     10.0        6.0   \n",
      "13       0.0     34.0       7.0     5.0        3.0    1.0     70.0        9.0   \n",
      "14       0.0     10.0       2.0    29.0       34.0    6.0     20.0       32.0   \n",
      "15      35.0    216.0     234.0   401.0      650.0  159.0    781.0     1016.0   \n",
      "16       0.0      2.0       0.0     1.0        9.0    0.0      0.0        3.0   \n",
      "17       0.0      1.0       0.0     1.0       10.0    0.0      3.0        8.0   \n",
      "18       1.0     66.0       1.0    10.0        7.0   46.0     31.0       13.0   \n",
      "19       0.0      0.0       0.0     0.0        0.0    0.0      4.0        3.0   \n",
      "20       0.0      0.0       0.0     1.0        9.0  112.0     10.0       39.0   \n",
      "21       0.0      1.0      15.0     0.0        8.0    3.0      1.0        1.0   \n",
      "22       1.0      4.0       0.0     3.0        3.0    1.0     10.0       10.0   \n",
      "23       0.0      6.0       1.0     6.0        2.0    0.0      4.0        2.0   \n",
      "24       5.0    102.0      18.0    43.0       32.0   20.0    217.0      124.0   \n",
      "25       0.0      0.0       0.0     0.0        1.0    0.0      1.0        1.0   \n",
      "26       1.0      3.0       1.0    22.0        8.0    1.0      2.0       26.0   \n",
      "27       0.0      1.0       3.0     1.0       10.0   13.0      3.0       33.0   \n",
      "28       2.0     11.0     129.0   215.0       71.0  179.0    450.0      303.0   \n",
      "29       0.0      1.0       0.0     0.0        1.0    0.0      1.0        1.0   \n",
      "..       ...      ...       ...     ...        ...    ...      ...        ...   \n",
      "71       0.0      3.0       6.0     1.0        4.0    1.0     14.0        4.0   \n",
      "72       1.0     21.0       0.0     3.0        3.0    9.0    122.0       15.0   \n",
      "73       0.0      3.0       0.0     2.0        2.0    2.0      6.0        9.0   \n",
      "74       0.0      1.0       1.0     0.0        0.0    0.0      5.0        0.0   \n",
      "75       3.0    243.0      14.0    10.0       23.0    0.0    587.0       75.0   \n",
      "76       0.0    100.0       1.0     6.0        3.0    6.0    190.0       56.0   \n",
      "77       4.0    333.0       3.0     3.0       16.0   16.0    725.0      104.0   \n",
      "78       0.0      4.0       3.0     0.0        1.0    0.0     10.0        2.0   \n",
      "79       0.0     13.0       1.0     2.0        3.0    1.0     21.0       14.0   \n",
      "80       0.0      5.0       2.0     0.0        2.0    0.0      6.0        5.0   \n",
      "81       0.0      1.0      14.0     2.0        1.0    6.0      7.0        2.0   \n",
      "82       1.0     37.0     255.0     7.0        1.0    2.0     73.0       10.0   \n",
      "83       3.0     91.0      16.0     3.0        5.0    0.0    113.0       16.0   \n",
      "84       0.0      0.0       3.0     1.0        5.0    0.0      2.0        5.0   \n",
      "85       0.0      0.0       0.0     0.0        2.0    0.0      1.0        5.0   \n",
      "86       0.0     20.0       0.0     0.0        4.0    1.0     41.0        7.0   \n",
      "87       0.0     11.0       1.0     7.0        6.0   11.0     27.0       32.0   \n",
      "88       0.0      0.0       0.0     1.0        1.0    0.0      0.0        2.0   \n",
      "89       1.0      3.0       3.0    15.0        8.0   18.0     33.0       30.0   \n",
      "90       0.0      1.0       1.0     4.0       18.0    0.0      5.0       16.0   \n",
      "91       0.0      0.0       3.0     0.0        0.0    0.0      0.0        0.0   \n",
      "92       0.0      2.0       0.0    13.0        5.0    0.0      1.0       24.0   \n",
      "93       2.0      2.0       0.0     1.0        2.0    5.0     26.0       10.0   \n",
      "94       1.0      8.0       1.0     5.0        8.0   33.0      5.0       22.0   \n",
      "95       0.0     47.0       8.0     1.0        0.0    1.0     73.0        5.0   \n",
      "96       0.0      9.0      60.0     0.0        8.0    2.0     22.0        4.0   \n",
      "97       2.0      6.0       0.0     8.0       13.0   15.0      2.0       44.0   \n",
      "98       0.0      0.0       0.0     0.0        0.0    5.0      1.0        3.0   \n",
      "99       2.0      8.0      34.0     3.0       16.0    4.0     71.0       12.0   \n",
      "100     13.0    313.0     196.0    83.0      204.0  118.0    963.0      597.0   \n",
      "\n",
      "     Sport  Technologie  Wirtschaft  \n",
      "0      0.0        195.0        23.0  \n",
      "1      4.0          4.0        24.0  \n",
      "2      7.0          2.0        31.0  \n",
      "3      0.0         31.0        11.0  \n",
      "4      1.0        297.0       219.0  \n",
      "5      0.0         13.0         5.0  \n",
      "6      0.0          0.0         3.0  \n",
      "7     13.0          1.0        35.0  \n",
      "8      4.0         18.0       133.0  \n",
      "9      0.0          4.0         6.0  \n",
      "10     0.0          0.0         4.0  \n",
      "11     7.0          3.0         8.0  \n",
      "12    19.0          6.0         2.0  \n",
      "13     1.0          1.0        35.0  \n",
      "14     0.0         48.0        11.0  \n",
      "15   884.0        321.0       871.0  \n",
      "16     2.0          7.0         3.0  \n",
      "17     1.0          2.0         6.0  \n",
      "18     2.0         24.0        26.0  \n",
      "19     0.0          0.0         5.0  \n",
      "20     2.0          3.0        13.0  \n",
      "21     0.0         21.0        74.0  \n",
      "22     0.0         18.0         5.0  \n",
      "23     0.0         39.0         4.0  \n",
      "24    10.0         23.0       153.0  \n",
      "25     0.0          1.0         0.0  \n",
      "26     2.0          4.0         7.0  \n",
      "27     0.0          4.0        12.0  \n",
      "28   120.0        108.0        62.0  \n",
      "29     2.0          2.0         2.0  \n",
      "..     ...          ...         ...  \n",
      "71     9.0          4.0        11.0  \n",
      "72     1.0          3.0         1.0  \n",
      "73    12.0          0.0         4.0  \n",
      "74     0.0          2.0         4.0  \n",
      "75     1.0         18.0        80.0  \n",
      "76    27.0          5.0         9.0  \n",
      "77     2.0          1.0         5.0  \n",
      "78     1.0          3.0        15.0  \n",
      "79     1.0          1.0         1.0  \n",
      "80     0.0          1.0         1.0  \n",
      "81     0.0         13.0        82.0  \n",
      "82     2.0         45.0        47.0  \n",
      "83     9.0          1.0        40.0  \n",
      "84     3.0         33.0         7.0  \n",
      "85     2.0          2.0         0.0  \n",
      "86     1.0          0.0         0.0  \n",
      "87     0.0          2.0         3.0  \n",
      "88     0.0          0.0         1.0  \n",
      "89    25.0         18.0         6.0  \n",
      "90     0.0          1.0         7.0  \n",
      "91     0.0          2.0         0.0  \n",
      "92     6.0          0.0         2.0  \n",
      "93     0.0          2.0         1.0  \n",
      "94     4.0          3.0         3.0  \n",
      "95     2.0          1.0         8.0  \n",
      "96     2.0          8.0       132.0  \n",
      "97     0.0         15.0         3.0  \n",
      "98     1.0         17.0         2.0  \n",
      "99     0.0          7.0        89.0  \n",
      "100  224.0        259.0       740.0  \n",
      "\n",
      "[101 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "int_target = le.fit_transform(target)\n",
    "\n",
    "target_map = np.zeros([101, num_models], dtype=float)\n",
    "for pred_topic, cur_target in zip(topic_for_article, int_target):\n",
    "    target_map[pred_topic][cur_target] += 1\n",
    "\n",
    "#for row in range(100):\n",
    "#    target_map[row] = target_map[row] / np.sum(target_map[row])\n",
    "    \n",
    "result = DataFrame(target_map, range(101), le.classes_)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-30T14:42:03.001084",
     "start_time": "2017-01-30T14:42:02.990940"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average homogenity: 0.440342218905\n"
     ]
    }
   ],
   "source": [
    "homogenity = np.zeros([101], dtype=float)\n",
    "\n",
    "#weights_map = Counter(target)\n",
    "#weights = np.asarray([weights_map[cur_class] for cur_class in le.classes_], dtype=int)\n",
    "#exclude last row because it is \"unknown\"\n",
    "for row in range(100):\n",
    "    weighted_row = target_map[row]# * weights\n",
    "    max_index = np.argmax(weighted_row)\n",
    "    max_value = weighted_row[max_index]\n",
    "    row_sum = np.sum(weighted_row)\n",
    "    homogenity[row] = max_value / row_sum if row_sum > 0 else 0\n",
    "    \n",
    "print(\"average homogenity: {}\".format(np.average(homogenity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
