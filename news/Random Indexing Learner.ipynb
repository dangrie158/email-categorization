{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T16:36:23.570857",
     "start_time": "2017-02-20T16:36:23.565841"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T17:36:26.886151",
     "start_time": "2017-02-20T17:36:26.881609"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 1000\n",
    "sparsity = 0.01\n",
    "index_set = (-1, 1)\n",
    "context_size = (2, 2)\n",
    "data_path = 'data/corpusAktuell.validation copy.txt'\n",
    "#data_path = '../wiki/data/wiki.de.txt'\n",
    "data = LineSentence(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T17:46:58.938396",
     "start_time": "2017-02-20T17:46:58.855185"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.utils import SaveLoad\n",
    "\n",
    "import logging\n",
    "\n",
    "class RandomIndexing(SaveLoad):\n",
    "    \n",
    "    def __init__(self, data, dims, sparsity, index_set, context_size):\n",
    "        \n",
    "        self.data = data\n",
    "        self.dims = dims\n",
    "        self.sparsity = sparsity\n",
    "        self.index_set = index_set\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        self.create_dict(data)\n",
    "        self.initialize_word_vectors(len(self.dict))\n",
    "        \n",
    "        self.learn(data, self.dict, self.index_vectors)\n",
    "        \n",
    "    def create_dict(self, data):\n",
    "        self.dict = Dictionary(data)\n",
    "        \n",
    "    def initialize_word_vectors(self, wc):\n",
    "        self.index_vectors = sparse.csr_matrix((wc, self.dims), dtype=np.int8)\n",
    "        \n",
    "        def urn_sampler(length):\n",
    "            return np.random.choice(self.index_set, size=length)\n",
    "        \n",
    "        for i in xrange(wc):\n",
    "            random_sparse_vector = sparse.random(1, self.dims, density = self.sparsity, data_rvs=urn_sampler)\n",
    "            self.index_vectors[i] = random_sparse_vector\n",
    "    \n",
    "    def learn(self, data, dict, index_vectors):\n",
    "        self.word_vectors = np.zeros((len(dict), self.dims), dtype=np.int32)\n",
    "        \n",
    "        def get_context(document, index, before, after):\n",
    "            context = []\n",
    "            for n in range(max(index - before, 0), index):\n",
    "                context.append(document[n])\n",
    "                \n",
    "            for n in range(index + 1, min(index + after + 1, len(document))):\n",
    "                context.append(document[n])\n",
    "                \n",
    "            return context\n",
    "        \n",
    "        for document in data:\n",
    "            #precompute document as a list of IDs in the dict \n",
    "            document_by_id = [self.dict.token2id[word] for word in document]\n",
    "            \n",
    "            print document\n",
    "            for n, word in enumerate(document_by_id):\n",
    "                context_words = get_context(document_by_id, n, *self.context_size)\n",
    "                word_vec = self.word_vectors[word]\n",
    "                \n",
    "                for context in context_words:\n",
    "                    index_vec = self.index_vectors[context]\n",
    "                    word_vec += index_vec\n",
    "                    \n",
    "                self.word_vectors[word] = word_vec\n",
    "        print(self.word_vectors.shape)\n",
    "                      \n",
    "    def __contains__(self, word):\n",
    "        return word in self.dict\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.word_vectors[self.dict.token2id[word]]\n",
    "                \n",
    "    def save(self, *args, **kwargs):\n",
    "        super(RandomIndexing, self).save(*args, **kwargs)\n",
    "\n",
    "    save.__doc__ = SaveLoad.save.__doc__\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, *args, **kwargs):\n",
    "        model = super(RandomIndexing, cls).load(*args, **kwargs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T17:47:09.461766",
     "start_time": "2017-02-20T17:47:09.004892"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 17:47:09,011 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-20 17:47:09,013 : INFO : built Dictionary(264 unique tokens: [u'wochen', u'hingegen', u'rechts', u'formal', u'regierungslager']...) from 1 documents (total 402 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'masendemonstrationen', u'fordern', u'den', u'rucktritt', u'der', u'prasidentin', u'staatsanwaltschaft', u'hat', u'anklage', u'gegen', u'drei', u'ihrer', u'engsten', u'vertrauten', u'erhoben', u'in', u'sudkorea', u'spitzt', u'sich', u'die', u'regierungskrise', u'weiter', u'zu', u'am', u'samstag', u'ging', u'erneut', u'fast', u'eine', u'million', u'menschen', u'im', u'ganzen', u'land', u'auf', u'die', u'stra\\xdfe', u'um', u'den', u'rucktritt', u'der', u'rechts', u'konservativen', u'prasidentin', u'park', u'geun', u'hye', u'zu', u'fordern', u'das', u'schreibt', u'unter', u'anderem', u'die', u'korea', u'times', u'in', u'der', u'landeshauptstadt', u'seoul', u'nahmen', u'danach', u'allein', u'600', u'000', u'menschen', u'an', u'den', u'protesten', u'teil', u'und', u'weitere', u'350', u'000', u'in', u'diversen', u'stadten', u'im', u'ganzen', u'land', u'unterdessen', u'hat', u'die', u'sudkoreanische', u'staatsanwaltschaft', u'nach', u'einem', u'bericht', u'der', u'chinesischen', u'nachrichtenagentur', u'xinhua', u'erklart', u'park', u'stehe', u'unter', u'verdacht', u'der', u'komplzenschaft', u'mit', u'drei', u'personen', u'aus', u'ihrem', u'unmittelbaren', u'umfeld', u'gegen', u'die', u'derzeit', u'ermittelt', u'wird', u'im', u'zentrum', u'des', u'skandals', u'der', u'das', u'land', u'seit', u'wochen', u'in', u'atem', u'halt', u'steht', u'die', u'langzeitfreundin', u'der', u'prasidentin', u'choi', u'soon', u'sil', u'deren', u'vater', u'schon', u'berater', u'von', u'parks', u'vater', u'zu', u'dessen', u'zeit', u'als', u'militardiktator', u'gewesen', u'ist', u'choi', u'werden', u'versuchter', u'betrug', u'erpressung', u'und', u'versuchte', u'erpressung', u'sowie', u'machtmissbrauch', u'vorgeworfen', u'ahnliche', u'vorwurfe', u'richten', u'sich', u'auch', u'gegen', u'den', u'ehemaligen', u'ersten', u'sekretar', u'der', u'prasidentin', u'der', u'inzwischen', u'in', u'untersuchungshaft', u'sitzt', u'choi', u'soll', u'park', u'gebeten', u'haben', u'uber', u'ahn', u'53', u'sudkoreanische', u'konzerne', u'unter', u'druck', u'zu', u'setzen', u'umgerechnet', u'uber', u'60', u'millionen', u'us', u'dollar', u'in', u'eine', u'von', u'choi', u'kontrollierte', u'stiftung', u'einzuzahlen', u'jeong', u'ho', u'seong', u'einem', u'weiteren', u'ehemaligem', u'sekretar', u'und', u'langzeitvertrauten', u'der', u'prasidentin', u'wird', u'au\\xdferdem', u'geheimnisverrat', u'vorgeworfen', u'es', u'wird', u'angenommen', u'dass', u'park', u'ihn', u'anwies', u'vertrauliche', u'militarische', u'und', u'diplomatische', u'dokumente', u'choi', u'zu', u'ubergeben', u'um', u'von', u'dieser', u'rat', u'einzuholen', u'der', u'haken', u'dabei', u'choi', u'bekleidet', u'keinerlei', u'regierungsamt', u'und', u'hatte', u'daher', u'keinen', u'einblick', u'in', u'die', u'unterlagen', u'haben', u'durfen', u'gegen', u'die', u'drei', u'wurde', u'am', u'sonntag', u'formal', u'anklage', u'erhoben', u'wie', u'die', u'korea', u'times', u'an', u'anderer', u'stelle', u'berichtet', u'die', u'prasidentin', u'selbst', u'ist', u'hingegen', u'fur', u'die', u'staatsanwalte', u'noch', u'nicht', u'fassbar', u'nach', u'sudkoreanischem', u'gesetz', u'genie\\xdft', u'sie', u'wahrend', u'ihrer', u'amtszeit', u'immunitat', u'das', u'waren', u'noch', u'15', u'weitere', u'monate', u'die', u'ankundigung', u'bedeutet', u'allerdings', u'dass', u'park', u'nach', u'ablauf', u'dieser', u'zeit', u'vor', u'gericht', u'gestellt', u'werden', u'konnte', u'denkbar', u'ist', u'auch', u'ein', u'amtsenthebungsverfahren', u'dafur', u'brauchte', u'die', u'opposition', u'jedoch', u'eine', u'zweidrittelmehrheit', u'im', u'parlament', u'und', u'musste', u'mindestens', u'20', u'stimmen', u'aus', u'dem', u'regierungslager', u'fur', u'einen', u'entsprechenden', u'antrag', u'gewinnen', u'die', u'konnte', u'es', u'vielleicht', u'geben', u'wenn', u'konservative', u'politiker', u'alle', u'felle', u'davon', u'schwimmen', u'sehen', u'und', u'um', u'ihre', u'wiederwahl', u'furchten', u'mussen', u'die', u'organisatoren', u'der', u'protest', u'haben', u'jedenfalls', u'schon', u'eine', u'neue', u'landesweite', u'demonstration', u'fur', u'das', u'nachste', u'wochenende', u'in', u'seoul', u'angekundigt', u'neue', u'nahrung', u'konnte', u'die', u'emporung', u'bekommen', u'wenn', u'die', u'staatsanwaltschaft', u'demnachst', u'die', u'prasidentin', u'erstmalig', u'zu', u'den', u'vorwurfen', u'gegen', u'ihre', u'freundin', u'und', u'andere', u'mitarbeiter', u'vernehmen', u'kann', u'kommentare', u'lesen', u'6', u'beitrage']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 17:47:09,432 : INFO : saving RandomIndexing object under data/ri.test.model, separately None\n",
      "2017-02-20 17:47:09,436 : INFO : saved data/ri.test.model\n",
      "2017-02-20 17:47:09,447 : INFO : loading RandomIndexing object from data/ri.test.model\n",
      "2017-02-20 17:47:09,449 : INFO : loading dict recursively from data/ri.test.model.dict.* with mmap=None\n",
      "2017-02-20 17:47:09,449 : INFO : loaded data/ri.test.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 1000)\n",
      "[ 0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0 -1\n",
      " -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  2  0  0  0  0  0  0  0  0  0  0  0  0 -2  0  0  1  0  0 -2  1  0  0  1\n",
      "  1  0  0  0 -1  0 -2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1\n",
      "  1  0  0  0  1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0 -1  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  2  0  0  0  2  0 -1  0  0 -1  0  0  1  0  0  0  2  0  0  0  1\n",
      "  0  0  0  0 -2  0  0  2  0  0  2 -1  0  0  0 -1  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0 -1  0 -1  0  0  0  0  0  1  1  0  0  0 -1 -1  1  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0 -1  0  0  1  0 -1 -2  0  0  0  0 -1  0  0  0\n",
      "  0  0  0  0  0  0  1 -1 -1  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0  1  0  0  2  1  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0 -1  0  0  1 -2  0  0 -1  0  0 -1  1  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0 -2  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  0  0  0  0  1 -1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0\n",
      "  0  0  0  0  0  0  1  0  0  1  0  0  0  1  0  0  0  0 -1  0  0  0  0 -1  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  0  1  0  0  0  0 -1  0  0  0  0  0  0  0  0  1  0\n",
      "  0 -1  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -2  0  0  0  0  0  0  1  0  0\n",
      "  0  0 -1  1  1  0  0  0  0  0  2  0  0  0  1  1  0  0  1  0  0  0  0  0  0\n",
      "  0  0  0 -1  1  0  0  0 -2  0  0  0  0 -1  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0 -2 -1  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  2 -2\n",
      "  0  0  0 -1  0  0  0  1  0  0  0  0  0  0  0  0  0  0 -1  1  0  0  0  0 -1\n",
      "  0  1  0  0  0  0  1  0  0  0  0  0  0  1 -1  0  0 -1  0 -1  0  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  3  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  2  0  0  0  0  0  1 -2  0  2  0  0  0  0  1 -1  1  0  0  0 -1\n",
      " -1  0  0  0  0  0  0  1  0  0  0  1 -1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0 -1  0 -1  0  0\n",
      " -2  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0 -1  0  0\n",
      "  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
      "[ 0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  1  1  0  0  0  0 -1\n",
      " -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  2  0  0  0  0  0  0  0  0  0  0  0  0 -2  0  0  1  0  0 -2  1  0  0  1\n",
      "  1  0  0  0 -1  0 -2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1\n",
      "  1  0  0  0  1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  2  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  0 -1  1  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  2  0  0  0  2  0 -1  0  0 -1  0  0  1  0  0  0  2  0  0  0  1\n",
      "  0  0  0  0 -2  0  0  2  0  0  2 -1  0  0  0 -1  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0 -1  0 -1  0  0  0  0  0  1  1  0  0  0 -1 -1  1  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0 -1  0  0  1  0 -1 -2  0  0  0  0 -1  0  0  0\n",
      "  0  0  0  0  0  0  1 -1 -1  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0  1  0  0  2  1  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  0\n",
      "  0  0  0  0  0  0  0 -1  0  0  1 -2  0  0 -1  0  0 -1  1  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  2  0  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0 -2  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  1  0  0  0  0  0  0  0  1 -1  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0  0\n",
      "  0  0  0  0  0  0  1  0  0  1  0  0  0  1  0  0  0  0 -1  0  0  0  0 -1  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  0  1  0  0  0  0 -1  0  0  0  0  0  0  0  0  1  0\n",
      "  0 -1  0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -2  0  0  0  0  0  0  1  0  0\n",
      "  0  0 -1  1  1  0  0  0  0  0  2  0  0  0  1  1  0  0  1  0  0  0  0  0  0\n",
      "  0  0  0 -1  1  0  0  0 -2  0  0  0  0 -1  0  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0 -2 -1  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  2 -2\n",
      "  0  0  0 -1  0  0  0  1  0  0  0  0  0  0  0  0  0  0 -1  1  0  0  0  0 -1\n",
      "  0  1  0  0  0  0  1  0  0  0  0  0  0  1 -1  0  0 -1  0 -1  0  0  0  0  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0 -2 -2  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  3  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  2  0  0  0  0  0  1 -2  0  2  0  0  0  0  1 -1  1  0  0  0 -1\n",
      " -1  0  0  0  0  0  0  1  0  0  0  1 -1  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0 -1  0 -1  0  0\n",
      " -2  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0 -1  0  0\n",
      "  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "model = RandomIndexing(data, d, sparsity, index_set, context_size)\n",
    "\n",
    "model.save('data/ri.test.model')\n",
    "\n",
    "print(model['den'])\n",
    "\n",
    "model = RandomIndexing.load('data/ri.test.model')\n",
    "\n",
    "print(model['den'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
