{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a CNN for news topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T00:25:50.357664",
     "start_time": "2017-01-21T00:25:50.347080"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set this to true to learn the model when running. false will load the model from disk\n",
    "LEARN=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T15:55:35.199273",
     "start_time": "2017-01-21T15:55:35.191753"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T12:17:20.136716",
     "start_time": "2017-01-21T12:16:28.899240"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel has 200 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "category_names = ['Sonstiges', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.200dim.word2vec.model')\n",
    "\n",
    "# we can precompute the L2-normalized vectors to save lots of memory\n",
    "# we can't continue learning after they are normalized but the model is static\n",
    "# in this usecase anyways\n",
    "base_model.init_sims(replace=True)\n",
    "\n",
    "k = base_model.vector_size\n",
    "print(\"basemodel has {} dimensional vectors\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** stopword filtering is commented out, check if filtering stopwords improves perfomance, but since the cnn learns \"patterns\" the filtering may distort the pattern too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:10:16.783587",
     "start_time": "2017-01-21T16:10:16.776045"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "    for name, path in paths:\n",
    "        articles_loaded = 0\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split()]# if x not in stopwords]\n",
    "                if len(tokens) > 0:\n",
    "                    X.append(tokens)\n",
    "                    y.append(name)\n",
    "\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:16:43.514779",
     "start_time": "2017-01-21T16:16:43.495268"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def equalize_a_posteriori(X, y):\n",
    "    # get the a posteriori for each class:\n",
    "    a_posteriori = {x: y.count(x) for x in set(y)}\n",
    "    print(\"p(C) for all classes (not normalized):\\n\")\n",
    "    for k,v in a_posteriori.iteritems():\n",
    "        print(\"{:<15}:{:>5}\".format(k, v))\n",
    "    max_posteriori = max(a_posteriori.values())\n",
    "    min_posteriori = min(a_posteriori.values())\n",
    "    \n",
    "    print(\"equalizing all training classes to argmin(p(C)) = {}\".format(min_posteriori))\n",
    "    \n",
    "    equal_X = []\n",
    "    equal_y = []\n",
    "    current_aposterioris = {c: 0 for c in a_posteriori.keys()}\n",
    "    \n",
    "    #sort the list descending by length of the article. this way we get rid of most of the padding\n",
    "    sorted_Xy = sorted(zip(X, y), key=lambda x: len(x[0]), reverse=True)\n",
    "    for cur_X, cur_y in sorted_Xy:\n",
    "        if(current_aposterioris[cur_y] <= min_posteriori):\n",
    "            equal_X.append(cur_X)\n",
    "            equal_y.append(cur_y)\n",
    "            current_aposterioris[cur_y] += 1 \n",
    "            \n",
    "    return equal_X, equal_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-25T11:26:15.096608",
     "start_time": "2016-12-25T11:21:21.000650"
    },
    "collapsed": true
   },
   "source": [
    "load the raw train and validation datasets in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:16:45.635139",
     "start_time": "2017-01-21T16:16:44.364927"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 13140 articles\n",
      "loaded 5594 articles\n",
      "p(C) for all classes (not normalized):\n",
      "\n",
      "Sonstiges      : 1789\n",
      "Lifestyle      :  775\n",
      "Wirtschaft     : 1960\n",
      "Finanzen       :  763\n",
      "Ausland        : 1122\n",
      "Lokal          :  583\n",
      "Politik        : 3772\n",
      "Sport          : 1047\n",
      "Technologie    :  837\n",
      "Kultur         :  492\n",
      "equalizing all training classes to argmin(p(C)) = 492\n",
      "after equalization, there are 4929 articles for training left\n"
     ]
    }
   ],
   "source": [
    "raw_train_X, raw_train_y = load_sets(train_paths)\n",
    "raw_validation_X, raw_validation_y = load_sets(validation_paths)\n",
    "\n",
    "raw_train_X, raw_train_y = equalize_a_posteriori(raw_train_X, raw_train_y)\n",
    "print(\"after equalization, there are {} articles for training left\".format(len(raw_train_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the average article length $N$\n",
    "\n",
    "$$N_{avg}=\\frac { \\sum _{ x\\in X }^{  }{ dim(x) }  }{ dim(X) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:19:16.016300",
     "start_time": "2017-01-21T16:19:16.003500"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average article length is: 827 words\n",
      "44 of 4929 articles get padded (0.892675999188%)\n",
      "average padding is 41 words\n"
     ]
    }
   ],
   "source": [
    "N_avg = sum([len(article) for article in raw_train_X]) / len(raw_train_X)\n",
    "print(\"average article length is: {} words\".format(N_avg))\n",
    "\n",
    "#override to avoid out-of-mem errors while learning\n",
    "N_avg = 200\n",
    "\n",
    "padding = [len(article) - N_avg for article in raw_train_X if len(article) - N_avg < 0]\n",
    "average_padding = -sum(padding) / len(padding)\n",
    "print(\"{} of {} articles get padded ({}%)\".format(len(padding), len(raw_train_X), len(padding) * 100.0 / len(raw_train_X)))\n",
    "print(\"average padding is {} words\".format(average_padding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-31T10:43:14.290958",
     "start_time": "2016-12-31T10:43:14.286344"
    }
   },
   "source": [
    "convert the raw input article to matrices of word-vectors.\n",
    "each article $x$ is represented as\n",
    "\n",
    "$${ x }_{ 1:n }={ x }_{ 1 }\\oplus { x }_{ 2 }\\oplus \\dots \\oplus { x }_{ n }$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ i }\\in { { R } }^{ k }$ is the $k$-dimensional word-embedding vector for the $i$-th word in the article.\n",
    "* $\\oplus$ is the concatenation operator\n",
    "\n",
    "the result is a matrix for each article in the form:\n",
    "$$x=\\begin{bmatrix} { x }_{ 1,1 } & { x }_{ 1,2 } & \\cdots  & { x }_{ 1,k } \\\\ { x }_{ 2,1 } & { x }_{ 2,2 } & \\cdots  & { x }_{ 2,k } \\\\ \\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ { x }_{ n,1 } & { x }_{ n,2 } & \\cdots  & { x }_{ n,k } \\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ n,k }$ is the value of the $k$-th dimension of the word-vector for word $n$\n",
    "\n",
    "the matrix is padded or cropped to a length of $N_{avg}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T15:03:01.383673",
     "start_time": "2017-01-22T15:02:53.035612"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scale <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-0d19d1ada5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticles_to_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mvalidation_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticles_to_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_validation_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-0d19d1ada5b2>\u001b[0m in \u001b[0;36marticles_to_matrices\u001b[0;34m(articles, word_dim, article_len)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mword_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# try to load the word from the basemodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.normal (numpy/random/mtrand/mtrand.c:19771)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: scale <= 0"
     ]
    }
   ],
   "source": [
    "def articles_to_matrices(articles, word_dim, article_len):\n",
    "    X = np.zeros((len(articles), article_len, word_dim), dtype='float32')\n",
    "    words_found = 0\n",
    "    words = 0\n",
    "    for x, raw_article in enumerate(articles):    \n",
    "        for x_n in range(N_avg):#while (words_found < N_avg):\n",
    "            # if the word can't be found, use a randomly initialized vector\n",
    "            # with the same distribution as the other vectors in the model\n",
    "            stddev = np.std(X)\n",
    "            mean = np.mean(X)\n",
    "            word_vec = np.random.normal(mean, stddev, word_dim)\n",
    "\n",
    "            # try to load the word from the basemodel\n",
    "            # TODO: maybe skip non-available words rather than default-zero\n",
    "            # so if more then N_avg words ar in the article they get used\n",
    "            try:\n",
    "                word_vec = base_model[raw_article[x_n % len(raw_article)]]\n",
    "                words_found += 1\n",
    "                X[x, x_n] = word_vec\n",
    "            except:\n",
    "                pass\n",
    "            words += 1\n",
    "    \n",
    "    words_skipped = words - words_found\n",
    "    print(\"skipped {} of {} words ({}%)\".format(words_skipped, words, words_skipped*100.0 / words))\n",
    "    return X\n",
    "\n",
    "train_X = articles_to_matrices(raw_train_X, k, N_avg)\n",
    "validation_X = articles_to_matrices(raw_validation_X, k, N_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the string train input data to a one-hot vector that can be used on the output layer of the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:22:37.581647",
     "start_time": "2017-01-21T16:22:37.556447"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             dim(V_i)\n",
      "Ausland           483\n",
      "Finanzen          333\n",
      "Kultur            208\n",
      "Lifestyle         328\n",
      "Lokal             249\n",
      "Politik          1596\n",
      "Sonstiges         770\n",
      "Sport             449\n",
      "Technologie       361\n",
      "Wirtschaft        817\n"
     ]
    }
   ],
   "source": [
    "def categories_to_one_hot(categories):\n",
    "    category_names, int_y = np.unique(categories, return_inverse=True)\n",
    "    y = np_utils.to_categorical(int_y)\n",
    "    return y, category_names\n",
    "\n",
    "train_y, _ = categories_to_one_hot(raw_train_y)\n",
    "target_y, category_names = categories_to_one_hot(raw_validation_y)\n",
    "\n",
    "stat = np.zeros(num_models, dtype=int)\n",
    "for row in target_y:\n",
    "    stat[np.argmax(row)] += 1\n",
    "\n",
    "validation_stats = DataFrame(stat, category_names, ['dim(V_i)'])\n",
    "print(validation_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model from (Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1408.5882]\n",
    "\n",
    "**Notes:**\n",
    "* Don't use L2 norm contraints on weight vectors (see (A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1510.03820]) (info from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow)\n",
    "* the input has 3 filter branches\n",
    "* the fully connected layer has ~~one~~ no hidden layer\n",
    "\n",
    "**Architecture** (from original paper):\n",
    "![Architecture](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:22:39.142014",
     "start_time": "2017-01-21T16:22:38.947252"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of filters of each size\n",
    "num_filters = 128\n",
    "# square filter sizes (3x3, 4x4, and 5x5)\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filter_branches = len(filter_sizes)\n",
    "\n",
    "# add the channel dimension (only 1 channel)\n",
    "# tip with np.expand_dims is from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "validation_X = np.expand_dims(validation_X, -1)\n",
    "\n",
    "# create the filter branches\n",
    "filter_branches = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    branch = Sequential()\n",
    "    branch.add(Convolution2D(num_filters, filter_size, k, init='uniform', border_mode='same',\n",
    "                        input_shape=train_X.shape[1:], W_regularizer=l2(0.01)))\n",
    "    branch.add(Activation('relu'))\n",
    "    pool_size =  num_filters;\n",
    "    branch.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    #branch.add(Dropout(0.25))\n",
    "    filter_branches.append(branch)\n",
    "\n",
    "# merge the branches by concatenating \n",
    "merged_filters = Merge(filter_branches, mode='concat')\n",
    "\n",
    "#create the final model with the filter layers and the fully connected layers\n",
    "model = Sequential()\n",
    "model.add(merged_filters)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_models))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model with an accuracy measurement\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-21T16:22:39.817394",
     "start_time": "2017-01-21T16:22:39.812658"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True, write_images=False))\n",
    "callbacks.append(ModelCheckpoint('checkpoints', monitor='acc', verbose=1, save_best_only=True, mode='max'))\n",
    "callbacks.append(EarlyStopping(monitor='loss', min_delta=0.01, patience=3, verbose=1, mode='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T00:27:42.489751",
     "start_time": "2017-01-21T16:22:41.145495"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4929 samples, validate on 5594 samples\n",
      "Epoch 1/1\n",
      "4864/4929 [============================>.] - ETA: 220s - loss: 2.2906 - acc: 0.1371Epoch 00000: acc improved from -inf to 0.13836, saving model to checkpoints\n",
      "4929/4929 [==============================] - 29098s - loss: 2.2896 - acc: 0.1384 - val_loss: 2.2588 - val_acc: 0.1566\n"
     ]
    }
   ],
   "source": [
    "# since we have more than one filter branch, tile the input data to all branches\n",
    "# np.tile causes the numpy kernel to crash (probably due to copying of the data. \n",
    "# putting it in a list works, because it then is just referenced 3 times)\n",
    "\n",
    "#tiled_train_X = np.tile(expanded_train_X, (num_filter_branches, 1))\n",
    "#tiled_validation_X = np.tile(expanded_validation_X, (num_filter_branches, 1))\n",
    "\n",
    "tiled_train_X = [train_X, train_X, train_X]\n",
    "tiled_validation_X = [validation_X, validation_X, validation_X]\n",
    "\n",
    "if LEARN:\n",
    "    model.fit(tiled_train_X, train_y, \n",
    "              validation_data=(tiled_validation_X, target_y),\n",
    "              nb_epoch=1, batch_size=128,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T00:27:42.692730",
     "start_time": "2017-01-22T00:27:42.535877"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if LEARN:\n",
    "    model.save('data/news.cnn.same-a-priori.model')\n",
    "else:\n",
    "    from keras.models import load_model\n",
    "    model = load_model('data/news.cnn.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T01:39:55.347327",
     "start_time": "2017-01-22T00:29:10.805343"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_y = model.predict([validation_X, validation_X, validation_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T01:39:55.402993",
     "start_time": "2017-01-22T01:39:55.348713"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: weight the output by the number of elements.\n",
    "# Why? because it may work although it won't make any sense...\n",
    "#predicted_y *= max(stat)\n",
    "#predicted_y /= stat\n",
    "\n",
    "#convert the log likelyhood prediction to a single-hot vector\n",
    "# with hotspot at index of highest likelihood\n",
    "predicted_y_singlehot = np.zeros(predicted_y.shape)\n",
    "for x, row in enumerate(predicted_y):\n",
    "    max_index = np.where(row == max(row))\n",
    "    predicted_y_singlehot[x, max_index] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T01:39:55.605319",
     "start_time": "2017-01-22T01:39:55.404601"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Ausland  Finanzen  Kultur  Lifestyle  Lokal  Politik  Sonstiges  \\\n",
      "Ausland          294       127      56        103     59      602        284   \n",
      "Finanzen           0        72       1          1      0      104         27   \n",
      "Kultur             0         0       0          0      0        0          0   \n",
      "Lifestyle         86        41      43        107     25      444        167   \n",
      "Lokal              0         8      19          2     78       33         16   \n",
      "Politik            0        21      33          7     52       90         26   \n",
      "Sonstiges         17         8      18         32      4       83         71   \n",
      "Sport              0         0       0          0      0        0          0   \n",
      "Technologie       81        52      37         68     29      211        167   \n",
      "Wirtschaft         5         4       1          8      2       29         12   \n",
      "\n",
      "             Sport  Technologie  Wirtschaft  \n",
      "Ausland        160          105         296  \n",
      "Finanzen         6            0           4  \n",
      "Kultur           1            0           0  \n",
      "Lifestyle       80           72         264  \n",
      "Lokal           27           11           0  \n",
      "Politik         59           12           0  \n",
      "Sonstiges       32           25          43  \n",
      "Sport            0            0           0  \n",
      "Technologie     74          131         177  \n",
      "Wirtschaft      10            5          33  \n"
     ]
    }
   ],
   "source": [
    "classification_matrix = np.zeros([num_models, num_models], dtype=int)\n",
    "\n",
    "for i in range(len(predicted_y_singlehot)):\n",
    "    predicted_index = np.where(predicted_y_singlehot[i] == 1)[0]\n",
    "    target_index = np.where(target_y[i] == 1)[0]\n",
    "    classification_matrix[predicted_index, target_index] += 1\n",
    "\n",
    "result = DataFrame(classification_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
