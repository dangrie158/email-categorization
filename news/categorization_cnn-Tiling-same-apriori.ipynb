{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a CNN for news topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:08:36.345386",
     "start_time": "2017-01-22T22:08:36.339858"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set this to true to learn the model when running. false will load the model from disk\n",
    "LEARN=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:08:40.795462",
     "start_time": "2017-01-22T22:08:38.131399"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:09:22.398799",
     "start_time": "2017-01-22T22:08:44.555470"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel has 200 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "category_names = ['Sonstiges', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.200dim.word2vec.model')\n",
    "\n",
    "# we can precompute the L2-normalized vectors to save lots of memory\n",
    "# we can't continue learning after they are normalized but the model is static\n",
    "# in this usecase anyways\n",
    "base_model.init_sims(replace=True)\n",
    "\n",
    "k = base_model.vector_size\n",
    "print(\"basemodel has {} dimensional vectors\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** stopword filtering is commented out, check if filtering stopwords improves perfomance, but since the cnn learns \"patterns\" the filtering may distort the pattern too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:09:26.193098",
     "start_time": "2017-01-22T22:09:26.182044"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "    for name, path in paths:\n",
    "        articles_loaded = 0\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split()]# if x not in stopwords]\n",
    "                if len(tokens) > 0:\n",
    "                    X.append(tokens)\n",
    "                    y.append(name)\n",
    "\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:09:28.272086",
     "start_time": "2017-01-22T22:09:28.251602"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def equalize_a_posteriori(X, y):\n",
    "    # get the a posteriori for each class:\n",
    "    a_posteriori = {x: y.count(x) for x in set(y)}\n",
    "    print(\"p(C) for all classes (not normalized):\\n\")\n",
    "    for k,v in a_posteriori.iteritems():\n",
    "        print(\"{:<15}:{:>5}\".format(k, v))\n",
    "    max_posteriori = max(a_posteriori.values())\n",
    "    min_posteriori = min(a_posteriori.values())\n",
    "    \n",
    "    print(\"equalizing all training classes to argmin(p(C)) = {}\".format(min_posteriori))\n",
    "    \n",
    "    equal_X = []\n",
    "    equal_y = []\n",
    "    current_aposterioris = {c: 0 for c in a_posteriori.keys()}\n",
    "    \n",
    "    #sort the list descending by length of the article. this way we get rid of most of the padding\n",
    "    sorted_Xy = sorted(zip(X, y), key=lambda x: len(x[0]), reverse=True)\n",
    "    for cur_X, cur_y in sorted_Xy:\n",
    "        if(current_aposterioris[cur_y] <= min_posteriori):\n",
    "            equal_X.append(cur_X)\n",
    "            equal_y.append(cur_y)\n",
    "            current_aposterioris[cur_y] += 1 \n",
    "            \n",
    "    return equal_X, equal_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-25T11:26:15.096608",
     "start_time": "2016-12-25T11:21:21.000650"
    },
    "collapsed": true
   },
   "source": [
    "load the raw train and validation datasets in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:09:31.876860",
     "start_time": "2017-01-22T22:09:29.715227"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 28883 articles\n",
      "loaded 3199 articles\n",
      "p(C) for all classes (not normalized):\n",
      "\n",
      "Sonstiges      : 4096\n",
      "Lifestyle      : 1796\n",
      "Wirtschaft     : 4417\n",
      "Finanzen       : 1427\n",
      "Ausland        : 2194\n",
      "Lokal          : 1374\n",
      "Politik        : 8147\n",
      "Sport          : 2316\n",
      "Technologie    : 1966\n",
      "Kultur         : 1150\n",
      "equalizing all training classes to argmin(p(C)) = 1150\n",
      "after equalization, there are 11509 articles for training left\n"
     ]
    }
   ],
   "source": [
    "raw_train_X, raw_train_y = load_sets(train_paths)\n",
    "raw_validation_X, raw_validation_y = load_sets(validation_paths)\n",
    "\n",
    "raw_train_X, raw_train_y = equalize_a_posteriori(raw_train_X, raw_train_y)\n",
    "print(\"after equalization, there are {} articles for training left\".format(len(raw_train_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the average article length $N$\n",
    "\n",
    "$$N_{avg}=\\frac { \\sum _{ x\\in X }^{  }{ dim(x) }  }{ dim(X) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:09:35.469373",
     "start_time": "2017-01-22T22:09:35.451166"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average article length is: 788 words\n",
      "414 of 11509 articles get padded (3.59718481189%)\n",
      "average padding is 77 words\n"
     ]
    }
   ],
   "source": [
    "N_avg = sum([len(article) for article in raw_train_X]) / len(raw_train_X)\n",
    "print(\"average article length is: {} words\".format(N_avg))\n",
    "\n",
    "#override to avoid out-of-mem errors while learning\n",
    "N_avg = 200\n",
    "\n",
    "padding = [len(article) - N_avg for article in raw_train_X if len(article) - N_avg < 0]\n",
    "average_padding = -sum(padding) / len(padding)\n",
    "print(\"{} of {} articles get padded ({}%)\".format(len(padding), len(raw_train_X), len(padding) * 100.0 / len(raw_train_X)))\n",
    "print(\"average padding is {} words\".format(average_padding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-31T10:43:14.290958",
     "start_time": "2016-12-31T10:43:14.286344"
    }
   },
   "source": [
    "convert the raw input article to matrices of word-vectors.\n",
    "each article $x$ is represented as\n",
    "\n",
    "$${ x }_{ 1:n }={ x }_{ 1 }\\oplus { x }_{ 2 }\\oplus \\dots \\oplus { x }_{ n }$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ i }\\in { { R } }^{ k }$ is the $k$-dimensional word-embedding vector for the $i$-th word in the article.\n",
    "* $\\oplus$ is the concatenation operator\n",
    "\n",
    "the result is a matrix for each article in the form:\n",
    "$$x=\\begin{bmatrix} { x }_{ 1,1 } & { x }_{ 1,2 } & \\cdots  & { x }_{ 1,k } \\\\ { x }_{ 2,1 } & { x }_{ 2,2 } & \\cdots  & { x }_{ 2,k } \\\\ \\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ { x }_{ n,1 } & { x }_{ n,2 } & \\cdots  & { x }_{ n,k } \\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ n,k }$ is the value of the $k$-th dimension of the word-vector for word $n$\n",
    "\n",
    "the matrix is padded or cropped to a length of $N_{avg}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:10:00.556039",
     "start_time": "2017-01-22T22:09:43.494043"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped 223084 of 2301800 words (9.69171952385%)\n",
      "skipped 62459 of 639800 words (9.76226945921%)\n"
     ]
    }
   ],
   "source": [
    "def articles_to_matrices(articles, word_dim, article_len):\n",
    "    X = np.zeros((len(articles), article_len, word_dim), dtype='float32')\n",
    "    words_found = 0\n",
    "    words = 0\n",
    "    \n",
    "    #stddev = np.std(base_model[:100])\n",
    "    #mean = np.mean(base_model[:100])\n",
    "            \n",
    "    for x, raw_article in enumerate(articles):    \n",
    "        for x_n in range(N_avg):#while (words_found < N_avg):\n",
    "            \n",
    "            word_vec = np.zeros(word_dim)\n",
    "            # try to load the word from the basemodel\n",
    "            # TODO: maybe skip non-available words rather than default-zero\n",
    "            # so if more then N_avg words ar in the article they get used\n",
    "            try:\n",
    "                word_vec = base_model[raw_article[x_n % len(raw_article)]]\n",
    "                words_found += 1\n",
    "            except:\n",
    "                # if the word can't be found, use a randomly initialized vector\n",
    "                # with the same distribution as the other vectors in the model\n",
    "                #word_vec = np.random.normal(mean, stddev, word_dim)\n",
    "                word_vec = base_model.seeded_vector(words)\n",
    "            X[x, x_n] = word_vec\n",
    "            words += 1\n",
    "    \n",
    "    words_skipped = words - words_found\n",
    "    print(\"skipped {} of {} words ({}%)\".format(words_skipped, words, words_skipped*100.0 / words))\n",
    "    return X\n",
    "\n",
    "train_X = articles_to_matrices(raw_train_X, k, N_avg)\n",
    "validation_X = articles_to_matrices(raw_validation_X, k, N_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the string train input data to a one-hot vector that can be used on the output layer of the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:10:14.074905",
     "start_time": "2017-01-22T22:10:14.050298"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             dim(V_i)\n",
      "Ausland           240\n",
      "Finanzen          159\n",
      "Kultur            129\n",
      "Lifestyle         203\n",
      "Lokal             151\n",
      "Politik           890\n",
      "Sonstiges         447\n",
      "Sport             262\n",
      "Technologie       216\n",
      "Wirtschaft        502\n"
     ]
    }
   ],
   "source": [
    "def categories_to_one_hot(categories):\n",
    "    category_names, int_y = np.unique(categories, return_inverse=True)\n",
    "    y = np_utils.to_categorical(int_y)\n",
    "    return y, category_names\n",
    "\n",
    "train_y, _ = categories_to_one_hot(raw_train_y)\n",
    "target_y, category_names = categories_to_one_hot(raw_validation_y)\n",
    "\n",
    "stat = np.zeros(num_models, dtype=int)\n",
    "for row in target_y:\n",
    "    stat[np.argmax(row)] += 1\n",
    "\n",
    "validation_stats = DataFrame(stat, category_names, ['dim(V_i)'])\n",
    "print(validation_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model from (Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1408.5882]\n",
    "\n",
    "**Notes:**\n",
    "* Don't use L2 norm contraints on weight vectors (see (A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1510.03820]) (info from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow)\n",
    "* the input has 3 filter branches\n",
    "* the fully connected layer has ~~one~~ no hidden layer\n",
    "\n",
    "**Architecture** (from original paper):\n",
    "![Architecture](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:10:19.850278",
     "start_time": "2017-01-22T22:10:19.656137"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of filters of each size\n",
    "num_filters = 128\n",
    "# square filter sizes (3x3, 4x4, and 5x5)\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filter_branches = len(filter_sizes)\n",
    "\n",
    "# add the channel dimension (only 1 channel)\n",
    "# tip with np.expand_dims is from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "validation_X = np.expand_dims(validation_X, -1)\n",
    "\n",
    "# create the filter branches\n",
    "filter_branches = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    branch = Sequential()\n",
    "    branch.add(Convolution2D(num_filters, filter_size, k, init='uniform', border_mode='same',\n",
    "                        input_shape=train_X.shape[1:], W_regularizer=l2(0.01)))\n",
    "    branch.add(Activation('relu'))\n",
    "    pool_size =  num_filters;\n",
    "    branch.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    #branch.add(Dropout(0.25))\n",
    "    filter_branches.append(branch)\n",
    "\n",
    "# merge the branches by concatenating \n",
    "merged_filters = Merge(filter_branches, mode='concat')\n",
    "\n",
    "#create the final model with the filter layers and the fully connected layers\n",
    "model = Sequential()\n",
    "model.add(merged_filters)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_models))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model with an accuracy measurement\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T22:10:22.288374",
     "start_time": "2017-01-22T22:10:22.283486"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks.append(TensorBoard(log_dir='logs', histogram_freq=0, write_graph=True, write_images=False))\n",
    "callbacks.append(ModelCheckpoint('checkpoints', monitor='acc', verbose=1, save_best_only=True, mode='max'))\n",
    "callbacks.append(EarlyStopping(monitor='loss', min_delta=0.01, patience=3, verbose=1, mode='auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T12:30:33.487309",
     "start_time": "2017-01-22T22:10:24.633998"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11509 samples, validate on 3199 samples\n",
      "Epoch 1/1\n",
      "11392/11509 [============================>.] - ETA: 436s - loss: 2.2491 - acc: 0.1533Epoch 00000: acc improved from -inf to 0.15414, saving model to checkpoints\n",
      "11509/11509 [==============================] - 51606s - loss: 2.2481 - acc: 0.1541 - val_loss: 2.2918 - val_acc: 0.1147\n"
     ]
    }
   ],
   "source": [
    "# since we have more than one filter branch, tile the input data to all branches\n",
    "# np.tile causes the numpy kernel to crash (probably due to copying of the data. \n",
    "# putting it in a list works, because it then is just referenced 3 times)\n",
    "\n",
    "#tiled_train_X = np.tile(expanded_train_X, (num_filter_branches, 1))\n",
    "#tiled_validation_X = np.tile(expanded_validation_X, (num_filter_branches, 1))\n",
    "\n",
    "tiled_train_X = [train_X, train_X, train_X]\n",
    "tiled_validation_X = [validation_X, validation_X, validation_X]\n",
    "\n",
    "if LEARN:\n",
    "    model.fit(tiled_train_X, train_y, \n",
    "              validation_data=(tiled_validation_X, target_y),\n",
    "              nb_epoch=1, batch_size=128,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T13:12:28.331502",
     "start_time": "2017-01-23T13:12:28.280396"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if LEARN:\n",
    "    model.save('data/news.cnn.same-a-priori.model')\n",
    "else:\n",
    "    from keras.models import load_model\n",
    "    model = load_model('data/news.cnn.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T13:54:22.506218",
     "start_time": "2017-01-23T13:12:29.167868"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_y = model.predict([validation_X, validation_X, validation_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T14:26:56.829078",
     "start_time": "2017-01-23T14:26:56.790553"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: weight the output by the number of elements.\n",
    "# Why? because it may work although it won't make any sense...\n",
    "#predicted_y *= max(stat)\n",
    "#predicted_y /= stat\n",
    "\n",
    "#convert the log likelyhood prediction to a single-hot vector\n",
    "# with hotspot at index of highest likelihood\n",
    "predicted_y_singlehot = np.zeros(predicted_y.shape)\n",
    "for x, row in enumerate(predicted_y):\n",
    "    max_index = np.where(row == max(row))\n",
    "    predicted_y_singlehot[x, max_index] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-23T14:26:57.518745",
     "start_time": "2017-01-23T14:26:57.300373"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Ausland  Finanzen  Kultur  Lifestyle  Lokal  Politik  Sonstiges  \\\n",
      "Ausland          240       116     100        176     79      641        394   \n",
      "Finanzen           0        29       1          0      0       39         10   \n",
      "Kultur             0         0       0          0      0        0          3   \n",
      "Lifestyle          0         0       0         19      0      118          1   \n",
      "Lokal              0        14      28          8     72       85         39   \n",
      "Politik            0         0       0          0      0        7          0   \n",
      "Sonstiges          0         0       0          0      0        0          0   \n",
      "Sport              0         0       0          0      0        0          0   \n",
      "Technologie        0         0       0          0      0        0          0   \n",
      "Wirtschaft         0         0       0          0      0        0          0   \n",
      "\n",
      "             Sport  Technologie  Wirtschaft  \n",
      "Ausland        208          202         415  \n",
      "Finanzen         3            0           0  \n",
      "Kultur           0            0           0  \n",
      "Lifestyle        0            1          87  \n",
      "Lokal           51           13           0  \n",
      "Politik          0            0           0  \n",
      "Sonstiges        0            0           0  \n",
      "Sport            0            0           0  \n",
      "Technologie      0            0           0  \n",
      "Wirtschaft       0            0           0  \n"
     ]
    }
   ],
   "source": [
    "classification_matrix = np.zeros([num_models, num_models], dtype=int)\n",
    "\n",
    "for i in range(len(predicted_y_singlehot)):\n",
    "    predicted_index = np.where(predicted_y_singlehot[i] == 1)[0]\n",
    "    target_index = np.where(target_y[i] == 1)[0]\n",
    "    classification_matrix[predicted_index, target_index] += 1\n",
    "\n",
    "result = DataFrame(classification_matrix, category_names, category_names)\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
