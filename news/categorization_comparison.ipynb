{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:03:48.684852",
     "start_time": "2017-01-22T20:03:47.366743"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords as sw\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from pandas import DataFrame\n",
    "\n",
    "stopwords = sw.words('german')\n",
    "\n",
    "# gensims LineSentence generator replaces umlauts with \n",
    "# u, a or o so add these variants to the stopwordlist\n",
    "for stopword in stopwords:\n",
    "    stopword = stopword.replace(u'ü', 'u')\n",
    "    stopword = stopword.replace(u'ö', 'o')\n",
    "    stopword = stopword.replace(u'ä', 'a')\n",
    "    if stopword not in stopwords:\n",
    "        stopwords.append(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:03:48.692783",
     "start_time": "2017-01-22T20:03:48.686692"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_names = ['Sonstiges', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of split corpora\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:03:48.708428",
     "start_time": "2017-01-22T20:03:48.695415"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split() if x not in stopwords]\n",
    "                if len(tokens) > 0:\n",
    "                    X.append(tokens)\n",
    "                    y.append(name)\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:07:54.276795",
     "start_time": "2017-01-22T20:03:48.712828"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 26689 articles\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = load_sets(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:07:54.345309",
     "start_time": "2017-01-22T20:07:54.279301"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mult_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "mult_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"multinomial nb\", MultinomialNB())])\n",
    "bern_nb_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM - which is supposed to be more or less state of the art \n",
    "# http://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf\n",
    "svc = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "svc_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"linear svc\", SVC(kernel=\"linear\"))])\n",
    "\n",
    "# random forrest classifier, since they are also used while validating the word2doc classification startegy\n",
    "rf = Pipeline([(\"count_vectorizer\", CountVectorizer(analyzer=lambda x: x)), (\"random forrest\", RandomForestClassifier(n_estimators=200))])\n",
    "rf_tfidf = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(analyzer=lambda x: x)), (\"random forrest\", RandomForestClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:52:54.742662",
     "start_time": "2017-01-22T20:07:54.347046"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training mult_nb\n",
      "now training mult_nb_tfidf\n",
      "now training bern_nb\n",
      "now training bern_nb_tfidf\n",
      "now training svc\n",
      "now training svc_tfidf\n",
      "now training rf\n",
      "now training rf_tfidf\n"
     ]
    }
   ],
   "source": [
    "all_models = [\n",
    "    (\"mult_nb\", mult_nb),\n",
    "    (\"mult_nb_tfidf\", mult_nb_tfidf),\n",
    "    (\"bern_nb\", bern_nb),\n",
    "    (\"bern_nb_tfidf\", bern_nb_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    (\"rf\", rf), \n",
    "    (\"rf_tfidf\", rf_tfidf)\n",
    "]\n",
    "\n",
    "for name, model in all_models:\n",
    "    print(\"now training {}\".format(name))\n",
    "    model.fit(train_X, train_y)\n",
    "  \n",
    "# do not use cross validation, because the other models are only validated with the single train/validation split\n",
    "# this way we make sure the scores really are compareable\n",
    "# cross validation code below:\n",
    "#scores = sorted([[name, cross_val_score(model, X, y, cv=5).mean()] \n",
    "#                 for name, model in all_models], \n",
    "#                key=lambda (_, x): -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:55:44.315950",
     "start_time": "2017-01-22T20:52:54.744626"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/ipykernel/__main__.py:7: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2959 articles\n",
      "predicting using mult_nb\n",
      "predicting using mult_nb_tfidf\n",
      "predicting using bern_nb\n",
      "predicting using bern_nb_tfidf\n",
      "predicting using svc\n",
      "predicting using svc_tfidf\n",
      "predicting using rf\n",
      "predicting using rf_tfidf\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "validate_X, validate_y = load_sets(validation_paths)\n",
    "\n",
    "for name, model in all_models:\n",
    "    print(\"predicting using {}\".format(name))\n",
    "    predicted_y = model.predict(validate_X)\n",
    "    score = accuracy_score(validate_y, predicted_y, normalize=True)\n",
    "    scores.append((name, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-22T20:55:44.332657",
     "start_time": "2017-01-22T20:55:44.317660"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model     score\n",
      "0        mult_nb  0.732004\n",
      "1  mult_nb_tfidf  0.492396\n",
      "2        bern_nb  0.625887\n",
      "3  bern_nb_tfidf  0.625887\n",
      "4            svc  0.743156\n",
      "5      svc_tfidf  0.784049\n",
      "6             rf  0.720514\n",
      "7       rf_tfidf  0.718824\n"
     ]
    }
   ],
   "source": [
    "result = DataFrame(scores, None, (\"model\", 'score'))\n",
    "print(result)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
