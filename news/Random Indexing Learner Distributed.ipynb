{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T21:32:44.193797",
     "start_time": "2017-02-20T21:32:44.190637"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T22:16:57.943574",
     "start_time": "2017-02-20T22:16:57.937947"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = 1000\n",
    "sparsity = 0.01\n",
    "index_set = (-1, 1)\n",
    "context_size = (2, 2)\n",
    "data_path = 'data/corpusAktuell.validation copy.txt'\n",
    "#data_path = '../wiki/data/wiki.de.txt'\n",
    "data = LineSentence(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T22:16:59.037922",
     "start_time": "2017-02-20T22:16:58.796811"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.utils import SaveLoad\n",
    "\n",
    "try:\n",
    "    from queue import Queue, Empty\n",
    "except ImportError:\n",
    "    from Queue import Queue, Empty\n",
    "\n",
    "import logging\n",
    "\n",
    "class RandomIndexing(SaveLoad):\n",
    "    \n",
    "    def __init__(self, data=None, dims=1000, sparsity=0.01, index_set=(-1, 1), context_size=(2,2), workers=1):\n",
    "        \n",
    "        self.data = data\n",
    "        self.dims = dims\n",
    "        self.sparsity = sparsity\n",
    "        self.index_set = index_set\n",
    "        self.context_size = context_size\n",
    "        self.workers = workers\n",
    "        \n",
    "        self.logger = logging.getLogger('RandomIndexing')\n",
    "        \n",
    "        self.dict = Dictionary()\n",
    "        \n",
    "        if data is not None:\n",
    "            self.learn(data, workers)\n",
    "        \n",
    "    def get_index_vectors(self, wc):\n",
    "        index_vectors = sparse.csr_matrix((wc, self.dims), dtype=np.int8)\n",
    "        \n",
    "        def urn_sampler(length):\n",
    "            return np.random.choice(self.index_set, size=length)\n",
    "        \n",
    "        for i in xrange(wc):\n",
    "            random_sparse_vector = sparse.random(1, self.dims, density = self.sparsity, data_rvs=urn_sampler)\n",
    "            index_vectors[i] = random_sparse_vector\n",
    "            \n",
    "        return index_vectors\n",
    "    \n",
    "    def learn(self, data, report_every=10000):\n",
    "        # add the documents to the dictionary\n",
    "        old_dict_len = len(self.dict)\n",
    "        self.dict.add_documents(data)\n",
    "        \n",
    "        if not hasattr(self, 'index_vectors'):\n",
    "            self.index_vectors = self.get_index_vectors(len(self.dict))\n",
    "        elif (len(self.dict) - old_dict_len) > 0:\n",
    "            new_index_vectors = self.get_index_vectors(len(self.dict) - old_dict_len)\n",
    "            self.index_vectors = sparse.vstack(self.index_vectors, new_index_vectors)\n",
    "            \n",
    "        # initialize the wordvectors if not already happened\n",
    "        if not hasattr(self, 'word_vectors'):\n",
    "           self.word_vectors = np.zeros((len(self.dict), self.dims), dtype=np.int32) \n",
    "        \n",
    "        job_queue = Queue(maxsize=100)\n",
    "        progress_queue = Queue(maxsize=self.workers)\n",
    "        worker_vector_spaces = np.zeros((self.workers, len(self.dict), self.dims), dtype=np.int32)\n",
    "                    \n",
    "        def get_context(document, index, before, after):\n",
    "            context = []\n",
    "            for n in range(max(index - before, 0), index):\n",
    "                context.append(document[n])\n",
    "                \n",
    "            for n in range(index + 1, min(index + after + 1, len(document))):\n",
    "                context.append(document[n])\n",
    "                \n",
    "            return context\n",
    "        \n",
    "        def train_worker(worker_num):\n",
    "            docs_processed = 0\n",
    "            while True:\n",
    "                document = job_queue.get()\n",
    "                \n",
    "                # queue is empty, kill this worker\n",
    "                if document is None:\n",
    "                    break;\n",
    "                    \n",
    "                docs_processed += 1\n",
    "                \n",
    "                if docs_processed % report_every == 0:\n",
    "                    progress_queue.put((worker_num, 'progress', docs_processed))\n",
    "                    \n",
    "                #precompute document as a list of IDs in the dict \n",
    "                document_by_id = [self.dict.token2id[word] for word in document]\n",
    "\n",
    "                for n, word in enumerate(document_by_id):\n",
    "                    context_words = get_context(document_by_id, n, *self.context_size)\n",
    "                    word_vec = worker_vector_spaces[worker_num, word]\n",
    "\n",
    "                    for context in context_words:\n",
    "                        index_vec = self.index_vectors[context]\n",
    "                        word_vec += index_vec\n",
    "\n",
    "                    worker_vector_spaces[worker_num, word] = word_vec\n",
    "                    \n",
    "            progress_queue.put((worker_num, 'done', docs_processed))\n",
    "                                        \n",
    "        def job_producer():\n",
    "            # fill in the queue\n",
    "            for document in data:\n",
    "                # put blocks until a space is available\n",
    "                job_queue.put(document)\n",
    "                \n",
    "            # put an empty element in the queue to notify the workers they are done\n",
    "            for _ in xrange(self.workers):\n",
    "                job_queue.put(None)\n",
    "                \n",
    "            \n",
    "        workers = []\n",
    "        workers.append(threading.Thread(target=job_producer))\n",
    "        workers += [threading.Thread(target=train_worker, args=(num,)) for num in xrange(self.workers)]\n",
    "        \n",
    "        \n",
    "        for thread in workers:\n",
    "            thread.daemon = True  # make interrupting the process with ctrl+c easier\n",
    "            thread.start()\n",
    "        self.logger.info('started {} jobs'.format(self.workers))\n",
    "        \n",
    "        unfinished_jobs = self.workers\n",
    "        worker_processed_documents = [0 for _ in xrange(self.workers)]\n",
    "        \n",
    "        while unfinished_jobs > 0:\n",
    "            # handle progress updates from the workers    \n",
    "            status = progress_queue.get();\n",
    "            if status[1] == 'done':\n",
    "                self.logger.info('job #{} finished. processed {} documents'.format(status[0], status[2]))\n",
    "                unfinished_jobs -= 1\n",
    "            elif status[1] == 'progress':\n",
    "                worker_processed_documents[status[0]] = status[2]\n",
    "                self.logger.info('processed {} documents'.format(sum(worker_processed_documents)))\n",
    "        \n",
    "        # add up all the worker vector spaces\n",
    "        new_vectors = np.sum(worker_vector_spaces, axis=0)\n",
    "        self.word_vectors = np.add(self.word_vectors, new_vectors)\n",
    "                      \n",
    "    def __contains__(self, word):\n",
    "        return word in self.dict\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.word_vectors[self.dict.token2id[word]]\n",
    "                \n",
    "    def save(self, *args, **kwargs):\n",
    "        # don't bother storing the cached normalized vectors, recalculable table\n",
    "        kwargs['ignore'] = kwargs.get('ignore', ['logger'])\n",
    "        super(RandomIndexing, self).save(*args, **kwargs)\n",
    "\n",
    "    save.__doc__ = SaveLoad.save.__doc__\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, *args, **kwargs):\n",
    "        model = super(RandomIndexing, cls).load(*args, **kwargs)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-20T22:17:01.197368",
     "start_time": "2017-02-20T22:16:59.301845"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-20 22:16:59,311 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2017-02-20 22:16:59,313 : INFO : built Dictionary(264 unique tokens: [u'wochen', u'hingegen', u'rechts', u'formal', u'regierungslager']...) from 1 documents (total 402 corpus positions)\n",
      "2017-02-20 22:16:59,526 : INFO : started 4 jobs\n",
      "2017-02-20 22:16:59,532 : INFO : job #2 finished. processed 0 documents\n",
      "2017-02-20 22:16:59,541 : INFO : job #3 finished. processed 0 documents\n",
      "2017-02-20 22:16:59,547 : INFO : job #1 finished. processed 0 documents\n",
      "2017-02-20 22:16:59,831 : INFO : job #0 finished. processed 1 documents\n",
      "2017-02-20 22:16:59,840 : INFO : adding document #0 to Dictionary(264 unique tokens: [u'wochen', u'hingegen', u'rechts', u'formal', u'regierungslager']...)\n",
      "2017-02-20 22:16:59,852 : INFO : built Dictionary(1842 unique tokens: [u'wochen', u'ostseekusten', u'regierungslager', u'gesehen', u'protest']...) from 12 documents (total 4811 corpus positions)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a32a7dc09bef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/corpusAktuell.validation.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/ri.test.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-459912f6ee47>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, data, report_every)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_dict_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mnew_index_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mold_dict_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_index_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# initialize the wordvectors if not already happened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniel/Library/Python/2.7/lib/python/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \"\"\"\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniel/Library/Python/2.7/lib/python/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;31m# check for fast path cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     if (N == 1 and format in (None, 'csr') and all(isinstance(b, csr_matrix)\n\u001b[0m\u001b[1;32m    553\u001b[0m                                                    for b in blocks.flat)):\n\u001b[1;32m    554\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compressed_sparse_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/daniel/Library/Python/2.7/lib/python/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m# Scalar other.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "model = RandomIndexing(workers= 4)\n",
    "#model.learn(data)\n",
    "\n",
    "data_path = 'data/corpusAktuell.validation.txt'\n",
    "data = LineSentence(data_path)\n",
    "model.learn(data)\n",
    "\n",
    "model.save('data/ri.test.model')\n",
    "\n",
    "print(model['trump'])\n",
    "\n",
    "model = RandomIndexing.load('data/ri.test.model')\n",
    "\n",
    "print(model['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
