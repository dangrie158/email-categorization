{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:17:53.481186",
     "start_time": "2017-01-01T14:17:51.772154"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Library/Python/2.7/lib/python/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Merge\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:18:48.902097",
     "start_time": "2017-01-01T14:17:53.483063"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basemodel has 400 dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "category_names = ['Sonstiges', 'Aktuell', 'Lifestyle', \n",
    "          'Wirtschaft', 'Finanzen', 'Ausland', 'Lokal', \n",
    "          'Politik', 'Sport', 'Technologie', 'Kultur']\n",
    "\n",
    "num_models = len(category_names)\n",
    "\n",
    "# the list of full corpora\n",
    "fulldata_paths = [(x, \"corpus/corpus{}.txt\".format(x)) for x in category_names]\n",
    "\n",
    "# the corpora with a fixed split for training and validation\n",
    "train_paths = [(x, \"data/corpus{}.training.txt\".format(x)) for x in category_names]\n",
    "validation_paths = [(x, \"data/corpus{}.validation.txt\".format(x)) for x in category_names]\n",
    "\n",
    "base_model = Word2Vec.load('../wiki/data/wiki.de.word2vec.model')\n",
    "\n",
    "# we can precompute the L2-normalized vectors to save lots of memory\n",
    "# we can't continue learning after they are normalized but the model is static\n",
    "# in this usecase anyways\n",
    "base_model.init_sims(replace=True)\n",
    "\n",
    "k = base_model.vector_size\n",
    "print(\"basemodel has {} dimensional vectors\".format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** stopword filtering is commented out, check if filtering stopwords improves perfomance, but since the cnn learns \"patterns\" the filtering may distort the pattern too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:18:48.910722",
     "start_time": "2017-01-01T14:18:48.904094"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sets(paths):\n",
    "    X, y = [], []\n",
    "\n",
    "    for name, path in paths:\n",
    "        with open(path) as cur_file:\n",
    "            for line in cur_file:\n",
    "                tokens = [x for x in line.split()]# if x not in stopwords]\n",
    "                X.append(tokens)\n",
    "                y.append(name)\n",
    "    print(\"loaded {} articles\".format(len(X)))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-25T11:26:15.096608",
     "start_time": "2016-12-25T11:21:21.000650"
    },
    "collapsed": true
   },
   "source": [
    "load the raw train and validation datasets in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:18:50.172608",
     "start_time": "2017-01-01T14:18:48.913328"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 28132 articles\n",
      "loaded 12042 articles\n"
     ]
    }
   ],
   "source": [
    "raw_train_X, raw_train_y = load_sets(train_paths)\n",
    "raw_validation_X, raw_validation_y = load_sets(validation_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the average article length $N$\n",
    "\n",
    "$$N_{avg}=\\frac { \\sum _{ x\\in X }^{  }{ dim(x) }  }{ dim(X) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:18:50.188938",
     "start_time": "2017-01-01T14:18:50.174661"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average article length is: 223 words\n"
     ]
    }
   ],
   "source": [
    "N_avg = 0\n",
    "N_avg = sum([len(article) for article in raw_train_X]) / len(raw_train_X)\n",
    "print(\"average article length is: {} words\".format(N_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-31T10:43:14.290958",
     "start_time": "2016-12-31T10:43:14.286344"
    }
   },
   "source": [
    "convert the raw input article to matrices of word-vectors.\n",
    "each article $x$ is represented as\n",
    "\n",
    "$${ x }_{ 1:n }={ x }_{ 1 }\\oplus { x }_{ 2 }\\oplus \\dots \\oplus { x }_{ n }$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ i }\\in { { R } }^{ k }$ is the $k$-dimensional word-embedding vector for the $i$-th word in the article.\n",
    "* $\\oplus$ is the concatenation operator\n",
    "\n",
    "the result is a matrix for each article in the form:\n",
    "$$x=\\begin{bmatrix} { x }_{ 1,1 } & { x }_{ 1,2 } & \\cdots  & { x }_{ 1,k } \\\\ { x }_{ 2,1 } & { x }_{ 2,2 } & \\cdots  & { x }_{ 2,k } \\\\ \\vdots  & \\vdots  & \\ddots  & \\vdots  \\\\ { x }_{ n,1 } & { x }_{ n,2 } & \\cdots  & { x }_{ n,k } \\end{bmatrix}$$\n",
    "\n",
    "where:\n",
    "* ${ x }_{ n,k }$ is the value of the $k$-th dimension of the word-vector for word $n$\n",
    "\n",
    "the matrix is padded or cropped to a length of $N_{avg}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:19:29.148199",
     "start_time": "2017-01-01T14:18:50.193331"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def articles_to_matrices(articles, word_dim, article_len):\n",
    "    X = np.zeros((len(articles), article_len, word_dim), dtype='float32')\n",
    "    for x, raw_article in enumerate(articles):\n",
    "        for x_n in range(article_len):\n",
    "            # if the word can't be found, use zero vector\n",
    "            word_vec = np.zeros(word_dim)\n",
    "            # try to load the word from the basemodel\n",
    "            # TODO: maybe skip non-available words rather than default-zero\n",
    "            # so if more then N_avg words ar in the article they get used\n",
    "            try:\n",
    "                word_vec = base_model[raw_article[x_n]]\n",
    "            except:\n",
    "                pass\n",
    "            X[x, x_n] = word_vec\n",
    "    return X\n",
    "\n",
    "train_X = articles_to_matrices(raw_train_X, k, N_avg)\n",
    "validation_X = articles_to_matrices(raw_validation_X, k, N_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the string train input data to a one-hot vector that can be used on the output layer of the cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-01T14:19:29.195190",
     "start_time": "2017-01-01T14:19:29.149642"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def categories_to_one_hot(categories):\n",
    "    _, int_y = np.unique(categories, return_inverse=True)\n",
    "    y = np_utils.to_categorical(int_y)\n",
    "    return y\n",
    "\n",
    "train_y = categories_to_one_hot(raw_train_y)\n",
    "target_y = categories_to_one_hot(raw_validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model from (Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1408.5882]\n",
    "\n",
    "**Notes:**\n",
    "* Don't use L2 norm contraints on weight vectors (see (A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification)[https://arxiv.org/abs/1510.03820]) (info from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow)\n",
    "* the input has 3 \n",
    "* the fully connected layer has one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-01T13:17:51.781Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of filters of each size\n",
    "num_filters = 128\n",
    "# square filter sizes (3x3, 4x4, and 5x5)\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filter_branches = len(filter_sizes)\n",
    "\n",
    "# add the channel dimension (only 1 channel)\n",
    "# tip with np.expand_dims is from http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow\n",
    "train_X = np.expand_dims(train_X, -1)\n",
    "validation_X = np.expand_dims(validation_X, -1)\n",
    "\n",
    "# create the filter branches\n",
    "filter_branches = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    branch = Sequential()\n",
    "    branch.add(Convolution2D(num_filters, filter_size, k, init='uniform', border_mode='same',\n",
    "                        input_shape=train_X.shape[1:]))\n",
    "    branch.add(Activation('relu'))\n",
    "    pool_size =  num_filters;\n",
    "    branch.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "    #branch.add(Dropout(0.25))\n",
    "    filter_branches.append(branch)\n",
    "\n",
    "# merge the branches by concatenating \n",
    "merged_filters = Merge(filter_branches, mode='concat')\n",
    "\n",
    "#create the final model with the filter layers and the fully connected layers\n",
    "model = Sequential()\n",
    "model.add(merged_filters)\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_models))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model with an accuracy measurement\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-01-01T13:17:51.782Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28132 samples, validate on 12042 samples\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# since we have more than one filter branch, tile the input data to all branches\n",
    "# np.tile causes the numpy kernel to crash (probably due to copying of the data. \n",
    "# putting it in a list works, because it then is just referenced 3 times)\n",
    "\n",
    "#tiled_train_X = np.tile(expanded_train_X, (num_filter_branches, 1))\n",
    "#tiled_validation_X = np.tile(expanded_validation_X, (num_filter_branches, 1))\n",
    "\n",
    "tiled_train_X = [train_X, train_X, train_X]\n",
    "tiled_validation_X = [validation_X, validation_X, validation_X]\n",
    "\n",
    "model.fit(tiled_train_X, train_y, \n",
    "          validation_data=(tiled_validation_X, target_y),\n",
    "          nb_epoch=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
